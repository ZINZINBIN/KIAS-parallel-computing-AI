
<!-- saved from url=(0041)https://computing.llnl.gov/tutorials/mpi/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Message Passing Interface (MPI)</title>

<script language="JavaScript" src="./Message Passing Interface (MPI)_files/tutorials.js.다운로드"></script>
<link rel="StyleSheet" href="./Message Passing Interface (MPI)_files/tutorials.css" type="text/css">
<link rel="SHORTCUT ICON" href="http://www.llnl.gov/favicon.ico">

<!-- BEGIN META TAGS -->
<meta name="LLNLRandR" content="UCRL-MI-133316">
<meta name="distribution" content="global">
<meta name="description" content="Livermore Computing Training">
<meta name="rating" content="general">
<meta http-equiv="keywords" content="Lawrence Livermore
National Laboratory, LLNL, High Performance Computing, parallel, programming, 
HPC, training, workshops, tutorials, Blaise Barney">
<meta name="copyright" content="This document is copyrighted U.S.
Department of Energy">
<meta name="Author" content="Blaise Barney">
<meta name="email" content="blaiseb@llnl.goclangv">
<!-- END META TAGS -->
</head>

<body>
<basefont size="3">            <!-- default font size -->
<font face="arial">

<!-- Begin Piwik Tracking Code  -->
<script src="./Message Passing Interface (MPI)_files/piwik.js.다운로드" type="text/javascript">
</script>
<script>
var siteName = document.domain;
var pkBaseURL = 'https://analytics.llnl.gov/';
if (typeof jQuery=="undefined") {
    document.write(unescape("%3Cscript src='" + pkBaseURL + "jquery.js' type='text/javascript'%3E%3C/script%3E"));
}
</script><script src="./Message Passing Interface (MPI)_files/jquery.js.다운로드" type="text/javascript"></script>
<script>
    try {
        var LLNLTracker = Piwik.getTracker(pkBaseURL + "piwik.php", 1);
        LLNLTracker.trackPageView();
        LLNLTracker.enableLinkTracking();
        var localSiteTracker = Piwik.getTracker(pkBaseURL + "piwik.php", 149);
        localSiteTracker.trackPageView();
        localSiteTracker.enableLinkTracking();
    }
    catch (err) {
        console.log(err);
    }
</script><noscript><p><img src="https://analytics.llnl.gov/piwik.php?idsite=149" style="border:0" alt="" /></p></noscript>
<!-- End Piwik Tracking Code -->

<a name="top">  </a>
<table cellpadding="0" cellspacing="0" width="100%">
<tbody><tr><td colspan="2" bgcolor="#3F5098">
  <table cellpadding="0" cellspacing="0" width="900">
  <tbody><tr><td background="./Message Passing Interface (MPI)_files/bg1.gif">
  <a name="top"> </a>
  <script language="JavaScript">addNavigation()</script>   <table border="0"><tbody><tr align="center" valign="center">    <td><font size="-1" face="arial">    <a href="https://hpc.llnl.gov/training/tutorials">Tutorials</a></font></td>    <td><b>|</b></td>   <td><font size="-1" face="arial">    <a href="https://computing.llnl.gov/tutorials/exercises/index.html">Exercises</a></font></td>    <td><b>|</b></td>   <td><font size="-1" face="arial">    <a href="https://computing.llnl.gov/tutorials/abstracts/index.html">Abstracts</a></font></td>    <td><b>|</b></td>   <td><font size="-1" face="arial">    <a href="https://hpc.llnl.gov/training/workshop-schedule">LC&nbsp;Workshops</a></font></td>    <td><b>|</b></td>   <td><font size="-1" face="arial">    <a href="https://computing.llnl.gov/tutorials/misc/comments.html">Comments</a></font></td>    <td><b>|</b></td>   <td><font size="-1" face="arial">   <a href="http://www.llnl.gov/disclaimer.html" target="W2">   Privacy &amp; Legal Notice</a></font></td>   </tr></tbody></table>   
  <p><br>
  </p><h1>Message Passing Interface (MPI)</h1>
  <p>
  </p></td></tr></tbody></table>
</td>
</tr><tr valign="top">
<td><i>Author: Blaise Barney, Lawrence Livermore National Laboratory</i></td>
<td align="right"><font size="-1">UCRL-MI-133316</font></td>
</tr></tbody></table>
<p>

<a name="TOC"> </a>
</p><h2>Table of Contents</h2>
<ol>
<li><a href="https://computing.llnl.gov/tutorials/mpi/#Abstract">Abstract</a>
</li><li><a href="https://computing.llnl.gov/tutorials/mpi/#What">What is MPI?</a>
</li><li><a href="https://computing.llnl.gov/tutorials/mpi/#LLNL">LLNL MPI Implementations and Compilers</a>
</li><li><a href="https://computing.llnl.gov/tutorials/mpi/#Getting_Started">Getting Started</a>
</li><li><a href="https://computing.llnl.gov/tutorials/mpi/#Environment_Management_Routines">Environment Management 
    Routines</a>
</li><li><a href="https://computing.llnl.gov/tutorials/mpi/#Exercise1">Exercise 1</a>
</li><li><a href="https://computing.llnl.gov/tutorials/mpi/#Point_to_Point_Routines"> Point to Point Communication 
    Routines</a>
    <ol>
    <li><a href="https://computing.llnl.gov/tutorials/mpi/#Point_to_Point_Routines">General Concepts</a>
    </li><li><a href="https://computing.llnl.gov/tutorials/mpi/#Routine_Arguments">MPI Message Passing Routine 
        Arguments</a>
    </li><li><a href="https://computing.llnl.gov/tutorials/mpi/#Blocking_Message_Passing_Routines">Blocking Message 
        Passing Routines</a>
    </li><li><a href="https://computing.llnl.gov/tutorials/mpi/#Non-Blocking_Message_Passing_Routines">Non-blocking Message 
        Passing Routines</a>
    </li></ol>
</li><li><a href="https://computing.llnl.gov/tutorials/mpi/#Exercise2">Exercise 2</a>
</li><li><a href="https://computing.llnl.gov/tutorials/mpi/#Collective_Communication_Routines">Collective Communication 
    Routines</a> 
</li><li><a href="https://computing.llnl.gov/tutorials/mpi/#Derived_Data_Types">Derived Data Types</a>
</li><li><a href="https://computing.llnl.gov/tutorials/mpi/#Group_Management_Routines">Group and Communicator Management 
    Routines</a>
</li><li><a href="https://computing.llnl.gov/tutorials/mpi/#Virtual_Topologies">Virtual Topologies</a>
</li><li><a href="https://computing.llnl.gov/tutorials/mpi/#MPI2-3">A Brief Word on MPI-2 and MPI-3</a>
</li><li><a href="https://computing.llnl.gov/tutorials/mpi/#Exercise3">Exercise 3</a>
</li><li><a href="https://computing.llnl.gov/tutorials/mpi/#References">References and More Information</a>
</li><li><a href="https://computing.llnl.gov/tutorials/mpi/#AppendixA">Appendix A: MPI-1 Routine Index</a>
</li></ol>
 
<!--------------------------------------------------------------------------->
 
<a name="Abstract"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Abstract</span>
</td></tr></tbody></table>
<p><br>

The Message Passing Interface Standard (MPI) is a message passing library
standard based on the consensus of the MPI Forum, which has over 40
participating organizations, including vendors, researchers, software library
developers, and users. The goal of the Message Passing Interface is to
establish a portable, efficient, and flexible standard for message passing
that will be widely used for writing message passing programs. As such, MPI
is the first standardized, vendor independent, message passing library. The
advantages of developing message passing software using MPI closely match the
design goals of portability, efficiency, and flexibility. MPI is not an
IEEE or ISO standard, but has in fact, become the "industry standard" for
writing message passing programs on HPC platforms.
</p><p>
The goal of this tutorial is to teach those unfamiliar with MPI how to
develop and run parallel programs according to the MPI standard.  The primary 
topics that are presented focus on those which are the most useful for 
new MPI programmers. The tutorial begins with an introduction, background, 
and basic information for getting started with MPI. This is followed by 
a detailed look at the MPI routines that are most useful for new MPI 
programmers, including MPI Environment Management, Point-to-Point 
Communications, and Collective Communications routines.  Numerous examples 
in both C and Fortran are provided, as well as a lab exercise.
</p><p>
The tutorial materials also include more advanced topics such as
Derived Data Types, Group and Communicator Management Routines, and
Virtual Topologies. However, these are not actually presented during the
lecture, but are meant to serve as "further reading" for those who are
interested. 
</p><p>
<i>Level/Prerequisites:</i> This tutorial is ideal for those who are new to parallel programming with MPI. A basic understanding of parallel programming in C or Fortran is required. For those who are unfamiliar with Parallel Programming in general, the material covered in 
<a href="https://computing.llnl.gov/tutorials/parallel_comp" target="_blank">EC3500: Introduction To Parallel 
Computing</a> would be helpful.
<br><br>

<!--------------------------------------------------------------------------->

<a name="What"> <br><br> </a> 
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">What is MPI?</span></td>
</tr></tbody></table>
</p><p><br>

<img src="./Message Passing Interface (MPI)_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">An Interface Specification:</span>
<img src="./Message Passing Interface (MPI)_files/MPIlogo2.gif" width="300" align="right" hspace="20">
</p><ul>
<p>
</p><li><b><font color="0056a8" size="+1">M P I</font></b> = 
<b><font color="0056a8" size="+1">M</font>essage 
<font color="0056a8" size="+1">P</font>assing 
<font color="0056a8" size="+1">I</font>nterface</b>
<p>
</p></li><li>MPI is a <i><b>specification</b></i>
    for the developers and users of message passing libraries.  By itself, it is NOT 
    a library - but rather the specification of what such a library should be. 
<p>
</p></li><li>MPI primarily addresses the <i><b>message-passing parallel programming
    model:</b></i> data is moved from the address space of one process to
    that of another process through cooperative operations on each process.
<p>
</p></li><li>Simply stated, the goal of the Message Passing Interface is to provide a
    widely used standard for writing message passing programs. The interface 
    attempts to be:
    <ul>
    <li>Practical
    </li><li>Portable
    </li><li>Efficient
    </li><li>Flexible
    </li></ul>
<p>
</p></li><li>The MPI standard has gone through a number of revisions, with the most
    recent version being MPI-3.x
<p>
</p></li><li>Interface specifications have been defined for C and Fortran90 language 
    bindings: 
    <ul>
    <li>C++ bindings from MPI-1 are removed in MPI-3
    </li><li>MPI-3 also provides support for Fortran 2003 and 2008 features
    </li></ul>
<p>
</p></li><li>Actual MPI library implementations differ in which version and features 
    of the MPI standard they support. Developers/users will need to be aware
    of this.
</li></ul>
<p>

<img src="./Message Passing Interface (MPI)_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Programming Model:</span>
</p><ul>
<p>
</p><li>Originally, MPI was designed for distributed memory architectures, which
    were becoming increasingly popular at that time (1980s - early 1990s).
<p>
<img src="./Message Passing Interface (MPI)_files/distributed_mem.gif" width="484" height="196" border="0">
</p><p>
</p></li><li>As architecture trends changed, shared memory SMPs were combined over
    networks creating hybrid distributed memory / shared memory systems. 
<p>
</p></li><li>MPI implementors adapted their libraries to handle both types of underlying
    memory architectures seamlessly.  They also adapted/developed ways of 
    handling different interconnects and protocols.
<p>
<img src="./Message Passing Interface (MPI)_files/hybrid_mem.gif" width="484" height="196" border="0">
</p><p>
</p><p>    
</p></li><li>Today, MPI runs on virtually any hardware platform:
    <ul>
    <li>Distributed Memory
    </li><li>Shared Memory
    </li><li>Hybrid
    </li></ul>  
<p>
</p></li><li>The programming model <u>clearly remains a distributed memory model</u> 
    however, regardless of the underlying physical architecture of the machine.
<p>
</p></li><li>All parallelism is explicit: the programmer is responsible for
    correctly identifying parallelism and implementing parallel 
    algorithms using MPI constructs.
</li></ul>
<p>

<img src="./Message Passing Interface (MPI)_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Reasons for Using MPI:</span>
</p><ul>
<p>
</p><li><b>Standardization</b>
    - MPI is the only message passing library that can be considered a standard. It 
    is supported on virtually all HPC platforms. Practically, it has replaced
    all previous message passing libraries.
<p>
</p></li><li><b>Portability</b>
    - There is little or no need to modify your source code
    when you port your application to a different platform that supports
    (and is compliant with) the MPI standard.
<p>
</p></li><li><b>Performance Opportunities</b>
    - Vendor implementations should be able to exploit native hardware features to 
    optimize performance. Any implementation is free to develop optimized
    algorithms.
<p>
</p></li><li><b>Functionality</b>
    - There are <b><i>over 430</i></b> routines defined in MPI-3, which includes the
    majority of those in MPI-2 and MPI-1.
<table border="0" cellpadding="0" cellspacing="0">
<tbody><tr valign="middle">
<td><img src="./Message Passing Interface (MPI)_files/note04.jpg" width="50" border="0"></td>
<td>&nbsp;Most MPI programs can be written using a dozen or less routines
</td>
</tr></tbody></table>

<p>
</p></li><li><b>Availability</b>
    - A variety of implementations are available, both vendor and 
    public domain.
</li></ul>
<p>

</p><p>
<img src="./Message Passing Interface (MPI)_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">History and Evolution:</span> (for those interested)
</p><ul>
<p>
</p><li>MPI has resulted from the efforts of numerous individuals and groups
    that began in 1992. Some history:
<p>
</p></li><li><b>1980s - early 1990s:</b> Distributed memory, parallel computing
    develops, as do a number of incompatible software tools for
    writing such programs - usually with tradeoffs between portability,
    performance, functionality and price.  Recognition of the need
    for a standard arose.
<p>
<img src="./Message Passing Interface (MPI)_files/MPIevolution.gif" width="438" height="320" border="1" align="right" hspace="20">
</p><p>
</p></li><li><b>Apr 1992:</b> Workshop on Standards for Message Passing in a 
    Distributed Memory Environment, sponsored by the Center for 
    Research on Parallel Computing, Williamsburg, Virginia.
    The basic features essential to a standard message passing 
    interface were discussed, and a working group established to
    continue the standardization process.  Preliminary draft proposal
    developed subsequently.
<p>
</p></li><li><b>Nov 1992:</b> Working group meets in Minneapolis.  MPI draft 
    proposal (MPI1) from ORNL presented.  Group adopts procedures 
    and organization to form the 
    <a href="https://computing.llnl.gov/tutorials/mpi/mpi.forum.html" target="_blank">MPI Forum.</a> 
    It eventually comprised of about 175 individuals from 
    40 organizations including parallel computer vendors, software 
    writers, academia and application scientists.  
<p>
</p></li><li><b>Nov 1993:</b> Supercomputing 93 conference - draft MPI standard 
    presented. 
<p>
</p></li><li><b>May 1994:</b> Final version of MPI-1.0 released
<ul>
<li>MPI-1.1 (Jun 1995)
</li><li>MPI-1.2 (Jul 1997)
</li><li>MPI-1.3 (May 2008).
</li></ul>
<p>
</p></li><li><b>1998:</b> MPI-2 picked up where the first MPI specification left off, and 
    addressed topics which went far beyond the MPI-1 specification.  
<ul>
<li>MPI-2.1 (Sep 2008)
</li><li>MPI-2.2 (Sep 2009)
</li></ul>
<p>
</p></li><li><b>Sep 2012:</b> The MPI-3.0 standard was approved. 
<ul>
<li>MPI-3.1 (Jun 2015)
</li></ul>
<p>
</p></li><li><b>Current:</b> The MPI-4.0 standard is under development.
</li></ul>
<p>

<img src="./Message Passing Interface (MPI)_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Documentation:</span>
</p><ul>
<p>
</p><li>Documentation for all versions of the MPI standard is available at: 
    <a href="http://www.mpi-forum.org/docs/" target="_blank">
    http://www.mpi-forum.org/docs/</a>.
</li></ul>

<!--------------------------------------------------------------------------->

<a name="LLNL"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">LLNL MPI Implementations and Compilers</span></td>
</tr></tbody></table>
<p><br>

<img src="./Message Passing Interface (MPI)_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Multiple Implementations:</span>
</p><ul>
<li>Although the MPI programming interface has been standardized, actual library 
    implementations will differ.
<p>
</p></li><li>For example, just a few considerations of many:
<ul>
<li>Which version of the MPI standard is supported?
</li><li>Are all of the features in a particular MPI version supported?
</li><li>Have any new features been added?  
</li><li>What network interfaces are supported?
</li><li>How are MPI applications compiled?
</li><li>How are MPI jobs launched?
</li><li>Runtime environment variable controls?
</li></ul>
<p>
</p></li><li>MPI library implementations on LC systems vary, as do the compilers they are built for. These are summarized in the table below:
<p>
<table border="1" cellpadding="5" cellspacing="0">
<tbody><tr valign="top">
<th>MPI Library</th>
<th>Where?</th>
<th>Compilers</th>
</tr><tr valign="top">
<td><b>MVAPICH</b></td>
<td>Linux clusters</td>
<td>GNU, Intel, PGI, Clang</td>
</tr><tr valign="top">
<td><b>Open MPI</b></td>
<td>Linux clusters</td>
<td>GNU, Intel, PGI, Clang</td>
</tr><tr valign="top">
<td><b>Intel MPI</b></td>
<td>Linux clusters</td>
<td>Intel, GNU</td>
</tr><tr valign="top">
<td><b>IBM Spectrum MPI</b></td>
<td>Coral Early Access and Sierra clusters</td>
<td>IBM, GNU, PGI, Clang</td>
</tr></tbody></table>
</p><p>
</p></li><li>Each MPI library is briefly discussed in the following sections, including links to additional detailed information.
</li></ul>
<p>

<img src="./Message Passing Interface (MPI)_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Selecting Your MPI Library and Compiler:</span>
</p><ul>
<li>LC provides a default MPI library for each cluster.
<p>
</p></li><li>LC also provides default compilers for each cluster.
<p>
</p></li><li>Typically, there are multiple versions of MPI libraries and compilers on each cluster.
<p>
</p></li><li>Modules are used to select a specific MPI library or compiler: More info <a href="https://hpc.llnl.gov/software/modules-and-software-packaging" target="_blank">HERE</a>.
<p>
</p></li><li>For example, using modules:
<ul>
<li>List which modules are currently loaded
</li><li>Show all available modules
</li><li>Load a different MPI module
</li><li>Load a different compiler module
</li><li>Confirm newly loaded modules
</li></ul>
<p>
<table border="1" cellpadding="5" cellspacing="0">
<tbody><tr valign="top">
<td><pre><font color="#AAAAAA">### List currently loaded modules</font>
% <font color="#DF4442">module list</font>

Currently Loaded Modules:
  1) intel/18.0.1   2) mvapich2/2.2   3) texlive/2016   4) StdEnv

<font color="#AAAAAA">### Show all available modules</font>
% <font color="#DF4442">module avail</font>

------------------------ /usr/tce/modulefiles/MPI/mvapich2/2.2 -------------------------
   openss/2.3

------------------ /usr/tce/modulefiles/MPI/intel/18.0.1/mvapich2/2.2 ------------------
   MUST/1.5        fftw/3.3.7              mpip/3.4.1
   boost/1.66.0    hdf5-parallel/1.8.18    pnetcdf/1.9.0

---------------------- /usr/tce/modulefiles/Compiler/intel/18.0.1 ----------------------
   hdf5-serial/1.8.18     impi/2018.0     (D)      netcdf-fortran/4.4.4
   hpctoolkit/10102016    mvapich2/2.2    (L,D)    openmpi/2.0.0        (D)
   impi/5.1.3             mvapich2/2.3             openmpi/2.1.0
   impi/2017.0            ncl_ncarg/6.3.0          openmpi/3.0.1
   impi/2017.1            netcdf/4.4.1.1

------------------------------ /usr/tce/modulefiles/Core -------------------------------
   StdEnv             (L)    inspector/2018.0    (D)      pgi/16.3
   advisor/2016.4            intel/14.0.3                 pgi/16.7
   advisor/2017.1            intel/15.0.6                 pgi/16.9
   advisor/2017.2            intel/16.0.2                 pgi/17.10
   advisor/2018.0            intel/16.0.3                 pgi/18.1              (D)
   advisor/2018.1     (D)    intel/16.0.4                 python/2.7.11
   allineaforge/6.0.5        intel/17.0.0                 python/2.7.13
   allineaforge/6.1.1        intel/17.0.2                 python/2.7.14         (D)
   allineaforge/7.0.3 (D)    intel/18.0-beta              python/3.5.1
   atom/1.13.1               intel/18.0.0                 python/3.6.0
   cbflib/0.9.2              intel/18.0.1        (L,D)    python/3.6.4
   clang/3.9.0               intel/18.0.2                 rasmol/2.7.5.2
   clang/3.9.1               intel/19.0-beta              spindle/0.10
   clang/4.0.0        (D)    iorun                        sqlcipher/3.7.9
   cmake/3.5.2        (D)    itac/2017.1                  stat/2.2.0
   cmake/3.8.2               itac/2018.0         (D)      stat/3.0.0            (D)
   cmake/3.9.2               launchmon/1.0.2              sublime_text/3.1.1
   cqrlib/1.0.5              ld-auto-rpath                svn/1.6.23
   cvector/1.0.3             license-llnl-ocf             svn/1.7.14            (D)
   dyninst/9.1.0             make/4.2.1                   tclap/1.2.0
   dyninst/9.3.1      (D)    mathematica/10.3.1           tecplot/2016.1
   emacs/24.3-redhat         matlab/8.1                   texlive/2016          (L)
   emacs/25.3         (D)    memcheckview/3.11.0          totalview/2016.01.06
   ensight/10.1.6            memcheckview/3.12.0          totalview/2016.04.08
   fgfs/1.1                  memcheckview/3.13.0 (D)      totalview/2016.06.21
   gcc/4.8-redhat            mesa3d/17.0.5                totalview/2016.07.22
   gcc/4.9.3          (D)    mkl/11.3.3                   totalview/2017X.01.07
   gcc/6.1.0                 mkl/2017.1                   totalview/2017.1.21
   gcc/7.1.0                 mkl/2018.0          (D)      totalview/2017.3.8    (D)
   gdal/1.9.0                mpa/1.1                      valgrind/3.11.0
   git/1.8.3.1               mpifileutils/0.6             valgrind/3.12.0
   git/2.8.3          (D)    mpifileutils/0.7    (D)      valgrind/3.13.0       (D)
   git-lfs/1.4.1             mrnet/5.0.1                  vampir/9.1.0
   glxgears/1.2              neartree/5.1.1               vampir/9.2            (D)
   gmt/5.1.2                 nvidia/375                   vmd/1.9.3
   gnuplot/5.0.0             opt                          vtune/2016.3
   grace/5.1.25              papi/5.4.3                   vtune/2017.1
   graphlib/2.0.0            paraview/5.0                 vtune/2018.0          (D)
   graphlib/3.0.0     (D)    paraview/5.4-server          xalt
   idl/8.5                   paraview/5.4        (D)      xforms/1.0.91
   inspector/2016.3          patchelf/0.8                 xsu
   inspector/2017.0          perfreports/7.0.3

-------------------------------- /usr/apps/modulefiles ---------------------------------
   Spheral/exp        Spheral/old       pact/current        pact/new
   Spheral/new (D)    pact/current_s    pact/new_s   (D)

------------------------ /usr/share/lmod/lmod/modulefiles/Core -------------------------
   lmod/6.5.1    settarg/6.5.1

  Where:
   L:  Module is loaded
   D:  Default Module

Use "module spider" to find all possible modules.
Use "module keyword key1 key2 ..." to search for all possible modules matching any of
the "keys".

<font color="#AAAAAA">### Load a different MPI module</font>
% <font color="#DF4442">module load openmpi/3.0.1</font>

Lmod is automatically replacing "mvapich2/2.2" with "openmpi/3.0.1"

<font color="#AAAAAA">### Load a different compiler module</font>
% <font color="#DF4442">module load pgi/18.1</font>

Lmod is automatically replacing "intel/18.0.1" with "pgi/18.1"

Due to MODULEPATH changes the following have been reloaded:
  1) openmpi/3.0.1

<font color="#AAAAAA">### Confirm newly loaded modules</font>
% <font color="#DF4442">module list</font>

Currently Loaded Modules:
  1) texlive/2016   2) StdEnv   3) pgi/18.1   4) openmpi/3.0.1
</pre></td>
</tr></tbody></table>
</p><p>
</p></li></ul>

<p><br></p><hr><p>
<!--------------------------------------------------------------------------->

</p><h2>MVAPICH</h2>
<img src="./Message Passing Interface (MPI)_files/mvapichLogo.gif" align="right">

<img src="./Message Passing Interface (MPI)_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">General Info:</span>
<ul>
<li>MVAPICH MPI is developed and supported by the 
    <a href="http://nowlab.cse.ohio-state.edu/" target="_blank">Network-Based 
    Computing Lab</a> at Ohio State University.
<p>
</p></li><li>Available on all of LC's Linux clusters.
<p>
</p></li><li>MVAPICH2
    <ul>
    <li>Default MPI implementation
    </li><li>Multiple versions available
    </li><li>MPI-2 and MPI-3 implementations based on MPICH MPI library from Argonne
        National Laboratory. Versions 1.9 and later implement MPI-3 according to the 
        developer's documentation.
    </li><li>Thread-safe
    </li></ul>
<p>
</p></li><li>To see what versions are available, and/or to select an alternate version, use
    <a href="https://computing.llnl.gov/tutorials/mpi/#Modules">Modules</a> commands. For example:
<pre><span class="cmd">module avail mvapich</span>         <i>(list available modules)</i>
<span class="cmd">module load mvapich2/2.3</span>     <i>(use the module of interest)</i></pre>
</li></ul>
<p>

<img src="./Message Passing Interface (MPI)_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Compiling:</span>
</p><ul>
<li>See the <a href="https://computing.llnl.gov/tutorials/mpi/#BuildScripts">MPI Build Scripts</a> table below.
</li></ul>
<p>

<img src="./Message Passing Interface (MPI)_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Running:</span>
</p><ul>
<li>MPI executables are launched using the SLURM <span class="cmd">srun</span> command
    with the appropriate options. For example, to launch an 8-process MPI job
    split across two different nodes in the <tt>pdebug</tt> pool:
<pre><span class="cmd">srun -N2 -n8 -ppdebug a.out</span></pre>
<p>
</p></li><li>The <span class="cmd">srun</span> command is discussed in detail in the 
    <a href="https://computing.llnl.gov/tutorials/linux_clusters/index.html#Starting" target="_blank">Running Jobs</a>
    section of the Linux Clusters Overview tutorial.
</li></ul>
<p>

<img src="./Message Passing Interface (MPI)_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Documentation:</span>
</p><ul>
<li>MVAPICH home page:
    <a href="http://mvapich.cse.ohio-state.edu/" target="_blank">
    mvapich.cse.ohio-state.edu/</a>
<p>
</p></li><li>MVAPICH2 User Guides:
    <a href="http://mvapich.cse.ohio-state.edu/userguide/" target="_blank">
    http://mvapich.cse.ohio-state.edu/userguide/</a>
</li></ul>

<p><br></p><hr><p>
<!--------------------------------------------------------------------------->

</p><h2>Open MPI</h2>
<img src="./Message Passing Interface (MPI)_files/openmpiLogo.gif" align="right">

<img src="./Message Passing Interface (MPI)_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">General Information:</span>
<ul>
<li>Open MPI is a thread-safe, open source MPI implementation 
    developed and supported by a consortium of academic, research, and 
    industry partners. 
<p>
</p></li><li>Available on all LC Linux clusters. However, you'll need to 
    load the desired 
<a href="https://computing.llnl.gov/tutorials/lc_resources/#Modules" target="_blank">module</a> first. For example:
<p> 
</p><pre><span class="cmd">module avail</span>                 <i>(list available modules)</i>
<span class="cmd">module load openmpi/3.0.1</span>    <i>(use the module of interest)</i></pre>
<p>
    This ensures that LC's MPI wrapper scripts point to the desired version
    of Open MPI.
</p></li></ul>
<p>

<img src="./Message Passing Interface (MPI)_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Compiling:</span>
</p><ul>
<li>See the <a href="https://computing.llnl.gov/tutorials/mpi/#BuildScripts">MPI Build Scripts</a> table below.
</li></ul>
<p>

<img src="./Message Passing Interface (MPI)_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Running:</span>
</p><ul>
<p>
</p><li>Be sure to load the same Open MPI module that you used to 
    build your executable. If you are running a batch job, you will need
    to load the module in your batch script.
<p>
</p></li><li>Launching an Open MPI job can be done using the following commands.
    For example, to run a 48 process MPI job:
<pre><span class="cmd">mpirun -np 48 a.out
mpiexec -np 48 a.out
srun -n 48 a.out</span></pre>
</li></ul>
<p>
<img src="./Message Passing Interface (MPI)_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Documentation:</span>
</p><ul>
<li>Open MPI home page:
    <a href="http://www.open-mpi.org/" target="_blank">http://www.open-mpi.org/</a>
</li></ul>

<p><br></p><hr><p>
<!--------------------------------------------------------------------------->

</p><h2>Intel MPI:</h2>
<ul>
<p>
</p><li>Available on LC's Linux clusters. 
<p>
</p></li><li>Based on MPICH3. Supports MPI-3 functionality.
<p>
</p></li><li>Thread-safe
<p>
</p></li><li>Compiling and running Intel MPI programs: see the
    LC documentation at: 
<a href="https://lc.llnl.gov/confluence/pages/viewpage.action?pageId=137725526" target="_blank">https://lc.llnl.gov/confluence/pages/viewpage.action?pageId=137725526</a>
</li></ul>

<p><br></p><hr><p>
<!--------------------------------------------------------------------------->

</p><h2>CORAL Early Access and Sierra Clusters:</h2>
<ul>
<p>
</p><li>The IBM Spectrum MPI library is the only supported implementation on these clusters. 
<p>
</p></li><li>Based on Open MPI. Includes MPI-3 functionality.
<p>
</p></li><li>Thread-safe
<p>
</p></li><li>NVIDIA GPU support
<p>
</p></li><li>Compiling and running IBM Spectrum MPI programs: see the Sierra Tutorial at
<a href="https://hpc.llnl.gov/training/tutorials/using-lcs-sierra-system" target="_blank">https://hpc.llnl.gov/training/tutorials/using-lcs-sierra-system</a></li></ul>

<p><br></p><hr><p>
<!--------------------------------------------------------------------------->
<a name="BuildScripts">  </a>

</p><h2>MPI Build Scripts</h2>
<ul>
<li>LC developed MPI compiler wrapper scripts are used to compile MPI programs on all LC systems.
<p>
</p></li><li>Automatically perform some error checks, include the appropriate 
    MPI #include files, link to the necessary MPI libraries, and pass options to 
    the underlying compiler.
</li></ul>

<p>
<table border="0" cellpadding="0" cellspacing="0">
<tbody><tr>
<td width="40"><img src="./Message Passing Interface (MPI)_files/warning2.jpg" width="31" height="30">
</td><td><b>Note:</b> you may need to load a <a href="https://computing.llnl.gov/tutorials/lc_resources/#Modules" target="_blank">module</a> for the desired MPI implementation, as discussed previously. Failing to do this will result in getting the default implementation.
</td></tr></tbody></table>
</p><ul>
<p>
</p><li>The table below lists the primary MPI compiler wrapper scripts for LC's Linux clusters. For CORAL EA / Sierra systems, see the links provided above.
<p><br>
<table border="1" cellspacing="0" cellpadding="3">
<tbody><tr>
<th colspan="4">MPI Build Scripts - Linux Clusters</th>
</tr><tr>
<th>Implementation</th>
<th>Language</th>
<th>Script Name</th>
<th>Underlying Compiler</th>

</tr><tr>
<td rowspan="5"><b>MVAPCH2</b></td>
<td><b>C</b></td>
<td><tt><b>mpicc</b></tt></td>
<td>C compiler for loaded compiler package</td>
</tr><tr>
<td><b>C++</b></td>
<td><tt><b>mpicxx
<br>mpic++</b></tt></td>
<td>C++ compiler for loaded compiler package</td>
</tr><tr>
<td rowspan="3"><b>Fortran</b></td>
<td><tt><b>mpif77</b></tt></td>
<td>Fortran77 compiler for loaded compiler package. Points to mpifort.</td>
</tr><tr>
<td><tt><b>mpif90</b></tt></td>
<td>Fortran90 compiler for loaded compiler package. Points to mpifort.</td>
</tr><tr>
<td><tt><b>mpifort</b></tt></td>
<td>Fortran 77/90 compiler for loaded compiler package.</td>

</tr><tr>
<td rowspan="5"><b>Open MPI</b></td>
<td><b>C</b></td>
<td><tt><b>mpicc</b></tt></td>
<td>C compiler for loaded compiler package</td>
</tr><tr>
<td><b>C++</b></td>
<td><tt><b>mpiCC<br>mpic++<br>mpicxx</b></tt></td>
<td>C++ compiler for loaded compiler package</td>
</tr><tr>
<td rowspan="3"><b>Fortran</b></td>
<td><tt><b>mpif77</b></tt></td>
<td>Fortran77 compiler for loaded compiler package. Points to mpifort.</td>
</tr><tr>
<td><tt><b>mpif90</b></tt></td>
<td>Fortran90 compiler for loaded compiler package. Points to mpifort.</td>
</tr><tr>
<td><tt><b>mpifort</b></tt></td>
<td>Fortran 77/90 compiler for loaded compiler package.</td>
</tr></tbody></table>
</p><p>
</p></li><li>For additional information:
    <ul>
    <li>See the man page (if it exists)
    </li><li>Issue the script name with the <tt>-help</tt> option
    </li><li>View the script yourself directly 
    </li></ul>
</li></ul>

<p><br></p><hr><p>
<!--------------------------------------------------------------------------->

</p><h2>Level of Thread Support</h2>
<ul>
<li>MPI libraries vary in their level of thread support:
    <ul>
    <p>
    </p><li><tt>MPI_THREAD_SINGLE</tt> - Level 0: Only one thread will execute. 
    <p>
    </p></li><li><tt>MPI_THREAD_FUNNELED</tt> - Level 1: 
        The process may be multi-threaded, but only
        the main thread will make MPI calls - all MPI calls are funneled 
        to the main thread.
    <p>
    </p></li><li><tt>MPI_THREAD_SERIALIZED</tt> - Level 2:
        The process may be multi-threaded, and 
        multiple threads may make MPI calls, but only one at a time. That is, 
        calls are not made concurrently from two distinct threads as all MPI calls
        are serialized. 
    <p>
    </p></li><li><tt>MPI_THREAD_MULTIPLE</tt> - Level 3: 
        Multiple threads may call MPI with no restrictions.
    </li></ul>
<p>
</p></li><li>Consult the <a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Init_thread.txt" target="_blank">
    <tt>MPI_Init_thread()</tt> man page</a> for details.
<p>
</p></li><li>A simple C language example for determining thread level support is shown below.
<p>
<table border="1" cellpadding="5" cellspacing="0" width="90%">
<tbody><tr valign="top">
<td><pre>#include "mpi.h"
#include &lt;stdio.h&gt;
 
int main( int argc, char *argv[] )
{
    int provided, claimed;
 
/*** Select one of the following
    MPI_Init_thread( 0, 0, MPI_THREAD_SINGLE, &amp;provided );
    MPI_Init_thread( 0, 0, MPI_THREAD_FUNNELED, &amp;provided );
    MPI_Init_thread( 0, 0, MPI_THREAD_SERIALIZED, &amp;provided );
    MPI_Init_thread( 0, 0, MPI_THREAD_MULTIPLE, &amp;provided );
***/ 

    MPI_Init_thread(0, 0, MPI_THREAD_MULTIPLE, &amp;provided );
    MPI_Query_thread( &amp;claimed );
        printf( "Query thread level= %d  Init_thread level= %d\n", claimed, provided );
 
    MPI_Finalize();
}

</pre>
Sample output:
<pre>Query thread level= 3  Init_thread level= 3

</pre></td>
</tr></tbody></table>
</p></li></ul>
<p>

<!--------------------------------------------------------------------------->

<a name="Getting_Started"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Getting Started</span></td>
</tr></tbody></table>
</p><p><br>

<img src="./Message Passing Interface (MPI)_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">General MPI Program Structure:</span>
</p><ul>
<p>
<table border="1" cellpadding="10" cellspacing="0">
<tbody><tr>
<td>
<img src="./Message Passing Interface (MPI)_files/prog_structure.gif">
</td>
<td><pre><font size="-1">
<font color="#DF4442">#include "mpi.h"</font>
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;

int main (int argc, char *argv[])
{
int numtasks, rank, dest, source, rc, count, tag=1;
char inmsg, outmsg='x';
<font color="#DF4442">MPI_Status Stat;</font>

<font color="#DF4442">MPI_Init(&amp;argc,&amp;argv);
MPI_Comm_size(MPI_COMM_WORLD, &amp;numtasks);
MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);</font>

if (rank == 0) {
  dest = 1;
  source = 1;
<font color="#DF4442">  rc = MPI_Send(&amp;outmsg, 1, MPI_CHAR, dest, tag, MPI_COMM_WORLD);
  rc = MPI_Recv(&amp;inmsg, 1, MPI_CHAR, source, tag, MPI_COMM_WORLD, &amp;Stat);</font>
  }

else if (rank == 1) {
  dest = 0;
  source = 0;
<font color="#DF4442">  rc = MPI_Recv(&amp;inmsg, 1, MPI_CHAR, source, tag, MPI_COMM_WORLD, &amp;Stat);
  rc = MPI_Send(&amp;outmsg, 1, MPI_CHAR, dest, tag, MPI_COMM_WORLD);</font>
  }

<font color="#DF4442">MPI_Finalize();</font>
}
</font></pre></td>
</tr></tbody></table>
</p></ul>
<img src="./Message Passing Interface (MPI)_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Header File:</span>
<ul>
<p> 
</p><li>Required for all programs that make MPI library calls.
<p>
<table border="1" cellspacing="0" cellpadding="5">
<tbody><tr align="CENTER">
<th>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;C include file&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</th>
<th>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Fortran include file&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</th>
</tr><tr>
<td><tt><b>#include  "mpi.h"         
</b></tt></td><td><tt><b>include   'mpif.h' </b></tt></td>   
</tr></tbody></table>
</p><p>
</p></li><li>With MPI-3 Fortran, the <tt><b>USE mpi_f08</b></tt> module is
    preferred over using the include file shown above.
</li></ul>
<p>

<img src="./Message Passing Interface (MPI)_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Format of MPI Calls:</span>
</p><ul>
<p> 
</p><li>C names are case sensitive; Fortran names are not.
<p>
</p></li><li>Programs must not declare variables or functions with names beginning with 
    the prefix MPI_ or PMPI_ (profiling interface).
<p>
<table border="1" cellspacing="0" cellpadding="5">
<tbody><tr align="CENTER">
<th colspan="2">C Binding</th>
</tr><tr>
<td bgcolor="#FOF5FE"><b>Format:
</b></td><td><tt><b><nobr>
    rc = MPI_Xxxxx(parameter, ... ) 
</nobr></b></tt></td></tr><tr>
<td bgcolor="#FOF5FE"><b>Example:
</b></td><td><tt><b><nobr>
    rc =  MPI_Bsend(&amp;buf,count,type,dest,tag,comm)
</nobr></b></tt></td></tr><tr>
<td bgcolor="#FOF5FE"><b>Error code:
</b></td><td>Returned as "rc". MPI_SUCCESS if successful
</td></tr><tr align="CENTER">
<th colspan="2">Fortran Binding</th>
</tr><tr>
<td bgcolor="#FOF5FE"><b>Format:
</b></td><td><tt><b><nobr>
    CALL MPI_XXXXX(parameter,..., ierr)<br> 
    call mpi_xxxxx(parameter,..., ierr) 
</nobr></b></tt></td></tr><tr>
<td bgcolor="#FOF5FE"><b>Example:
</b></td><td><tt><b><nobr>
    CALL MPI_BSEND(buf,count,type,dest,tag,comm,ierr)
</nobr></b></tt></td></tr><tr>
<td bgcolor="#FOF5FE"><b>Error code:
</b></td><td>Returned as "ierr" parameter. MPI_SUCCESS if successful
</td></tr></tbody></table>
</p></li></ul>
<p>

<img src="./Message Passing Interface (MPI)_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Communicators and Groups:</span>
</p><ul>
<p> 
</p><li>MPI uses objects called communicators and groups to define which
    collection of processes may communicate with each other.
<p>
</p></li><li>Most MPI routines require you to specify a communicator as an argument. 
<p>
</p></li><li>Communicators and groups will be covered in more detail later.  For now,
    simply use <b>MPI_COMM_WORLD</b> whenever a communicator is required - 
    it is the predefined communicator that includes all of your MPI processes. 
<p>
<table border="1" cellspacing="0" cellpadding="5"><tbody><tr><td>
<img src="./Message Passing Interface (MPI)_files/comm_world.gif" width="500" height="303" border="0"></td></tr></tbody></table>
</p></li></ul>
<p>

<img src="./Message Passing Interface (MPI)_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Rank:</span>
</p><ul>
<p> 
</p><li>Within a communicator, every process has its own unique, integer 
    identifier assigned by the system when the process initializes.  A rank 
    is sometimes also called a "task ID".  Ranks are contiguous and begin 
    at zero.
<p>
</p></li><li>Used by the programmer to specify the source and destination of
    messages.  Often used conditionally by the application to control
    program execution (if rank=0 do this / if rank=1 do that).
</li></ul>
<p>

<img src="./Message Passing Interface (MPI)_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Error Handling:</span>
</p><ul>
<p> 
</p><li>Most MPI routines include a return/error code parameter, as described
    in the "Format of MPI Calls" section above.
<p>
</p></li><li>However, according to the MPI standard, the default behavior of an MPI call is
    to abort if there is an error. This means you will probably not be able to 
    capture a return/error code other than MPI_SUCCESS (zero).
<p>
</p></li><li>The standard does provide a means to override this default error handler. 
    A discussion on how to do this is available <a href="https://computing.llnl.gov/tutorials/mpi/errorHandlers.pdf" target="_blank">HERE</a>. You can also consult the error handling section of the
    relevant MPI Standard documentation located at 
    <a href="http://www.mpi-forum.org/docs/" target="_blank">http://www.mpi-forum.org/docs/</a>.
<p>
</p></li><li>The types of errors displayed to the user are implementation dependent.
</li></ul>

<!--------------------------------------------------------------------------->

<a name="Environment_Management_Routines"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Environment Management Routines</span></td>
</tr></tbody></table>
<p><br>

This group of routines is used for interrogating and setting the MPI execution 
environment, and covers an assortment of purposes, such as initializing and terminating the MPI environment, querying a rank's identity, querying the MPI
library's version, etc. Most of the commonly used ones are described below.
</p><p>
<b><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Init.txt" target="_blank"> 
MPI_Init</a></b>
</p><ul>Initializes the MPI execution environment.  This function must be called
    in every MPI program, must be called before any other MPI functions
    and must be called only once in an MPI program.  For C programs, MPI_Init 
    may be used to pass the command line arguments to all processes,
    although this is not required by the standard and is implementation
    dependent.
<p>
<table border="1" cellpadding="5" cellspacing="0" width="75%">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Init (&amp;argc,&amp;argv) <br>
    MPI_INIT (ierr)
</b></tt></nobr></td></tr><tr></tr></tbody></table>
</p></ul><p><br>

<b><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Comm_size.txt" target="_blank"> 
MPI_Comm_size</a></b>
</p><p>
</p><ul>Returns the total number of MPI processes in the specified 
    communicator, such as MPI_COMM_WORLD. If the communicator is
    MPI_COMM_WORLD, then it represents the number of MPI tasks
    available to your application.
<p>
<table border="1" cellpadding="5" cellspacing="0" width="75%">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Comm_size (comm,&amp;size) <br>
    MPI_COMM_SIZE (comm,size,ierr)
</b></tt></nobr></td></tr><tr></tr></tbody></table>
</p></ul><p><br>

<b><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Comm_rank.txt" target="_blank"> 
MPI_Comm_rank</a></b>
</p><p>
</p><ul>Returns the rank of the calling MPI process within the specified communicator.  
    Initially, each process will be assigned a unique integer rank
    between 0 and number of tasks - 1 within the communicator
    MPI_COMM_WORLD.  This rank is often referred to as a task ID.  
    If a process becomes associated with other communicators, it will have
    a unique rank within each of these as well.
<p>
<table border="1" cellpadding="5" cellspacing="0" width="75%">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Comm_rank (comm,&amp;rank) <br>
    MPI_COMM_RANK (comm,rank,ierr) 
</b></tt></nobr></td></tr><tr></tr></tbody></table>
</p></ul><p><br>

<b><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Abort.txt" target="_blank"> 
MPI_Abort</a></b>
</p><p>
</p><ul>Terminates all MPI processes associated with the communicator.  In
    most MPI implementations it terminates ALL processes regardless of
    the communicator specified.
<p>
<table border="1" cellpadding="5" cellspacing="0" width="75%">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Abort (comm,errorcode)<br>
    MPI_ABORT (comm,errorcode,ierr) 
</b></tt></nobr></td></tr><tr></tr></tbody></table>
</p></ul><p><br>

<b><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Get_processor_name.txt" target="_blank"> 
MPI_Get_processor_name</a></b>
</p><p>
</p><ul>Returns the processor name. Also
    returns the length of the name.  The buffer for "name" must be at
    least MPI_MAX_PROCESSOR_NAME characters in size.  What is returned into
    "name" is implementation dependent - may not be the same as the output
    of the "hostname" or "host" shell commands.
<p>
<table border="1" cellpadding="5" cellspacing="0" width="75%">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Get_processor_name (&amp;name,&amp;resultlength)<br> 
    MPI_GET_PROCESSOR_NAME (name,resultlength,ierr)
</b></tt></nobr></td></tr><tr></tr></tbody></table>
</p></ul><p><br>

<b><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Get_version.txt" target="_blank"> 
MPI_Get_version</a></b> 
</p><p>
</p><ul>Returns the version and subversion of the MPI standard that's 
    implemented by the library.
<p>
<table border="1" cellpadding="5" cellspacing="0" width="75%">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Get_version (&amp;version,&amp;subversion)<br> 
    MPI_GET_VERSION (version,subversion,ierr)
</b></tt></nobr></td></tr><tr></tr></tbody></table>
</p></ul><p><br>

<b><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Initialized.txt" target="_blank"> 
MPI_Initialized</a></b>
</p><p>
</p><ul>Indicates whether MPI_Init has been called - returns flag as either
    logical true (1) or false(0).  MPI requires that MPI_Init
    be called once and only once by each process.  This may pose a problem
    for modules that want to use MPI and are prepared to call MPI_Init
    if necessary.  MPI_Initialized solves this problem.
<p>
<table border="1" cellpadding="5" cellspacing="0" width="75%">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Initialized (&amp;flag) <br>
    MPI_INITIALIZED (flag,ierr)
</b></tt></nobr></td></tr><tr></tr></tbody></table>
</p></ul><p><br>

<b><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Wtime.txt" target="_blank"> 
MPI_Wtime</a></b>
</p><p>
</p><ul>Returns an elapsed wall clock time in seconds (double precision) on the 
    calling processor.
<p>
<table border="1" cellpadding="5" cellspacing="0" width="75%">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Wtime ()<br>
    MPI_WTIME ()
</b></tt></nobr></td></tr><tr></tr></tbody></table>
</p></ul><p><br>

<b><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Wtick.txt" target="_blank"> 
MPI_Wtick</a></b>
</p><p>
</p><ul>Returns the resolution in seconds (double precision) of MPI_Wtime.
<p>
<table border="1" cellpadding="5" cellspacing="0" width="75%">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Wtick ()<br>
    MPI_WTICK ()
</b></tt></nobr></td></tr><tr></tr></tbody></table>
</p></ul><p><br>

<b><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Finalize.txt" target="_blank"> 
MPI_Finalize</a></b>
</p><p>
</p><ul>Terminates the MPI execution environment.  This function should be
    the last MPI routine called in every MPI program - no other MPI
    routines may be called after it.
<p>
<table border="1" cellpadding="5" cellspacing="0" width="75%">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Finalize ()<br>
    MPI_FINALIZE (ierr)
</b></tt></nobr></td></tr><tr></tr></tbody></table>
</p></ul><p><br>

</p><p></p><hr><p>

</p><h2>Examples: Environment Management Routines</h2>

<ul>
<p>
<table border="1" cellpadding="15" cellspacing="0" width="90%"><tbody><tr><td> <!---outer table--->
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tbody><tr>
<td bgcolor="FOF5FE" colspan="2" align="center"><img src="./Message Passing Interface (MPI)_files/page01.gif"></td>
<td bgcolor="FOF5FE"><b><br>&nbsp;&nbsp;&nbsp;&nbsp;
C Language - Environment Management Routines
</b><p></p></td>

</tr><tr valign="top"><td colspan="3"><p></p></td>

</tr><tr valign="top">
<td width="30"><pre><font color="#AAAAAA"> 1<br> 2<br> 3<br> 4<br> 5<br> 6<br> 7<br> 8<br> 9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28
</font></pre></td>

<td width="1" bgcolor="#7099cc"></td>

<td><pre>   <font color="#AAAAAA">// required MPI include file  </font>
   #include <font color="#DF4442">"mpi.h"</font>
   #include &lt;stdio.h&gt;

   int main(int argc, char *argv[]) {
   int  numtasks, rank, len, rc; 
   char hostname[MPI_MAX_PROCESSOR_NAME];

   <font color="#AAAAAA">// initialize MPI  </font>
   <font color="#DF4442">MPI_Init</font>(&amp;argc,&amp;argv);

   <font color="#AAAAAA">// get number of tasks </font>
   <font color="#DF4442">MPI_Comm_size</font>(MPI_COMM_WORLD,&amp;numtasks);

   <font color="#AAAAAA">// get my rank  </font>
   <font color="#DF4442">MPI_Comm_rank</font>(MPI_COMM_WORLD,&amp;rank);

   <font color="#AAAAAA">// this one is obvious  </font>
   <font color="#DF4442">MPI_Get_processor_name</font>(hostname, &amp;len);
   printf ("Number of tasks= %d My rank= %d Running on %s\n", numtasks,rank,hostname);

<font color="#AAAAAA">
        // do some work with message passing 
</font>

   <font color="#AAAAAA">// done with MPI  </font>
   <font color="#DF4442">MPI_Finalize();</font>
   }

</pre></td>
</tr></tbody></table>
</td></tr></tbody></table> <!---outer table--->

<br><br><br>

<table border="1" cellpadding="15" cellspacing="0" width="90%"><tbody><tr><td> <!---outer table--->
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tbody><tr>
<td width="30" bgcolor="FOF5FE" colspan="2" align="center"><img src="./Message Passing Interface (MPI)_files/page01.gif"></td>
<td bgcolor="FOF5FE"><b><br>&nbsp;&nbsp;&nbsp;&nbsp;
Fortran - Environment Management Routines
</b><p></p></td>

</tr><tr valign="top"><td colspan="3"><p></p></td>

</tr><tr valign="top">
<td width="30"><pre><font color="#AAAAAA"> 1<br> 2<br> 3<br> 4<br> 5<br> 6<br> 7<br> 8<br> 9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br></font></pre></td>

<td width="1" bgcolor="#7099cc"></td>

<td><pre>   program simple

<font color="#AAAAAA">   ! required MPI include file</font>
   include <font color="#DF4442">'mpif.h'</font>

   integer numtasks, rank, len, ierr  
   character(MPI_MAX_PROCESSOR_NAME) hostname

<font color="#AAAAAA">   ! initialize MPI</font>
   call <font color="#DF4442">MPI_INIT</font>(ierr)

<font color="#AAAAAA">   ! get number of tasks</font>
   call <font color="#DF4442">MPI_COMM_SIZE</font>(MPI_COMM_WORLD, numtasks, ierr)

<font color="#AAAAAA">   ! get my rank</font>
   call <font color="#DF4442">MPI_COMM_RANK</font>(MPI_COMM_WORLD, rank, ierr)

<font color="#AAAAAA">   ! this one is obvious</font>
   call <font color="#DF4442">MPI_GET_PROCESSOR_NAME</font>(hostname, len, ierr)
   print *, 'Number of tasks=',numtasks,' My rank=',rank,' Running on=',hostname

<font color="#AAAAAA">
        ! do some work with message passing 
</font>

<font color="#AAAAAA">   ! done with MPI</font>
   call <font color="#DF4442">MPI_FINALIZE</font>(ierr)

   end

</pre></td>
</tr></tbody></table>
</td></tr></tbody></table>  <!---outer table--->

</p></ul>

<!--------------------------------------------------------------------------->

<a name="Exercise1"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">MPI Exercise 1</span></td>
</tr></tbody></table>
<h2>Getting Started</h2>

<dd>
<table border="1" cellpadding="5" cellspacing="0" width="90%">
<tbody><tr valign="top"><td bgcolor="#FOF5FE" height="400">
<b><u>Overview:</u>
<ul>
<li>Login to an LC cluster using your workshop username and OTP token
</li><li>Copy the exercise files to your home directory
</li><li>Familiarize yourself with LC's MPI compilers
</li><li>Write a simple "Hello World" MPI program using several MPI
    Environment Management routines
</li><li>Successfully compile your program
</li><li>Successfully run your program - several different ways
</li></ul>
</b><p><b>
<img src="./Message Passing Interface (MPI)_files/point02.jpg" width="100" heigth="45" border="0">
<a href="https://computing.llnl.gov/tutorials/mpi/exercise.html" target="_blank">GO TO THE EXERCISE HERE</a>
</b>
</p><ul>
<h3>Approx. 20 minutes</h3>
</ul>
</td></tr></tbody></table>

</dd>

<!--------------------------------------------------------------------------->

<a name="Point_to_Point_Routines"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Point to Point Communication Routines</span></td>
</tr></tbody></table>
<h2>General Concepts</h2>

<img src="./Message Passing Interface (MPI)_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">First, a Simple Example:</span>
<p>
<table border="0" cellspacing="0" cellpadding="0">
<tbody><tr valign="top">
<td><ul>
<p>
</p><li>The value of PI can be calculated in various ways. Consider the Monte Carlo  
    method of approximating PI: 
<ul> 
<li>Inscribe a circle with radius <b>r</b> in a square with side length of <b>2<i>r</i></b> 
</li><li>The area of the circle is <b>Πr<sup>2</sup></b> and the area of the square is <b>4r<sup>2</sup></b> 
</li><li>The ratio of the area of the circle to the area of the square is: <br><b>Πr<sup>2</sup> / 4r<sup>2</sup> = Π / 4</b> 
</li><li>If you randomly generate <b>N</b> points inside the square, approximately <br><b>N * Π / 4</b> of those points (<b>M</b>) should fall inside the circle. </li><li><b>Π</b> is then approximated as: 
<br><b>N * Π / 4 = M 
<br>Π / 4 = M / N 
<br>Π = 4 * M / N</b> 
</li><li>Note that increasing the number of points generated improves the approximation.
</li></ul>
<p>
</p></li><li>Serial pseudo code for this procedure:
<p>
<table border="1" cellpadding="5" cellspacing="0">
<tbody><tr><td><pre><b>npoints = 10000
circle_count = 0

do j = 1,npoints
  generate 2 random numbers between 0 and 1
  xcoordinate = random1
  ycoordinate = random2
  if (xcoordinate, ycoordinate) inside circle
  then circle_count = circle_count + 1
end do

PI = 4.0*circle_count/npoints
</b></pre>
</td></tr></tbody></table>
</p><p>
</p></li><li>Leads to an "embarrassingly parallel" solution: 
    <ul>
    <li>Break the loop iterations into chunks that can be executed by different
        tasks simultaneously.
    </li><li>Each task executes its portion of the loop a number of times.
    </li><li>Each task can do its work without requiring any information
        from the other tasks (there are no data dependencies). 
    </li><li>Master task receives results from other tasks <b>using send/receive
        point-to-point operations</b>.
    </li></ul>
<p>
</p></li><li>Pseudo code solution:
    <font color="#DF4442"><b>red</b></font> highlights changes for 
    parallelism.
<p>
<table border="1" cellpadding="5" cellspacing="0">
<tbody><tr><td><pre><b>npoints = 10000
circle_count = 0
<font color="#DF4442">
p = number of tasks
num = npoints/p

find out if I am MASTER or WORKER </font>

do j = 1,<font color="#DF4442">num </font>
  generate 2 random numbers between 0 and 1
  xcoordinate = random1
  ycoordinate = random2
  if (xcoordinate, ycoordinate) inside circle
  then circle_count = circle_count + 1
end do
<font color="#DF4442">
if I am MASTER
  receive from WORKERS their circle_counts
  compute PI (use MASTER and WORKER calculations)
else if I am WORKER
  send to MASTER circle_count
endif</font></b></pre>
<p>
</p><p>
Example MPI Program in C: &nbsp;
<a href="https://computing.llnl.gov/tutorials/mpi/samples/C/mpi_pi_reduce.c" target="_blank">mpi_pi_reduce.c</a>
<br>
Example MPI Program in Fortran: &nbsp;
<a href="https://computing.llnl.gov/tutorials/mpi/samples/Fortran/mpi_pi_reduce.f" target="_blank">mpi_pi_reduce.f</a>
</p></td></tr></tbody></table>
</p><p>
</p></li><li><font style="background-color: yellow"><b>Key Concept:</b> Divide work between 
    available tasks which communicate data via point-to-point message passing 
    calls.</font>
</li></ul>
</td>

<td>
<img src="./Message Passing Interface (MPI)_files/pi1.gif" width="400" height="517" border="0" hspace="20">
<p>
<img src="./Message Passing Interface (MPI)_files/pi2.gif" width="400" height="475" border="0" hspace="20">
</p></td></tr></tbody></table>


<img src="./Message Passing Interface (MPI)_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Types of Point-to-Point Operations:</span>
</p><ul>
<li>MPI point-to-point operations typically involve message passing between
    two, and only two, different MPI tasks. One task is performing a send 
    operation and the other task is performing a matching receive operation.
<p>
</p></li><li>There are different types of send and receive routines used for 
    different purposes. For example:
    <ul>
    <li>Synchronous send
    </li><li>Blocking send / blocking receive
    </li><li>Non-blocking send / non-blocking receive
    </li><li>Buffered send
    </li><li>Combined send/receive
    </li><li>"Ready" send
    </li></ul>
<p>
</p></li><li>Any type of send routine can be paired with any type of receive routine.
<p>
</p></li><li>MPI also provides several routines associated with send - receive
    operations, such as those used to wait for a message's arrival or
    probe to find out if a message has arrived.
</li></ul>

<img src="./Message Passing Interface (MPI)_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Buffering:</span>
<ul>
<li>In a perfect world, every send operation would be perfectly
    synchronized with its matching receive. This is rarely the case.
    Somehow or other, the MPI implementation must be able to deal with
    storing data when the two tasks are out of sync.
<p>
</p></li><li>Consider the following two cases:
    <ul>
    <li>A send operation occurs 5 seconds before the receive is ready -
        where is the message while the receive is pending?
    </li><li>Multiple sends arrive at the same receiving task which can only
        accept one send at a time - what happens to the messages that are
        "backing up"?
    </li></ul>
<p>
</p></li><li>The MPI implementation (not the MPI standard) decides what happens to
    data in these types of cases.  Typically, a <b>system buffer</b> area
    is reserved to hold data in transit.  For example:
<p>
<img src="./Message Passing Interface (MPI)_files/buffer_recv.gif" width="608" height="384" border="0">
</p><p>
</p></li><li>System buffer space is:
    <ul>
    <li>Opaque to the programmer and managed entirely by the MPI library
    </li><li>A finite resource that can be easy to exhaust
    </li><li>Often mysterious and not well documented
    </li><li>Able to exist on the sending side, the receiving side, or both
    </li><li>Something that may improve program performance because it allows
        send - receive operations to be asynchronous.
    </li></ul>
<p>
</p></li><li>User managed address space (i.e. your program variables) is called 
    the <b>application buffer</b>. MPI also provides for a user managed
    send buffer.
</li></ul>

<img src="./Message Passing Interface (MPI)_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Blocking vs. Non-blocking:</span>
<ul>
<li>Most of the MPI point-to-point routines can be used in either blocking
    or non-blocking mode.
<p>
</p></li><li><b>Blocking:</b>
    <ul>
    <li>A blocking send routine will only "return" after it is safe to modify
        the application buffer (your send data) for reuse. Safe means that
        modifications will not affect the data intended for the receive task.
        Safe does not imply that the data was actually received - it may
        very well be sitting in a system buffer.
    </li><li>A blocking send can be synchronous which means there is handshaking
        occurring with the receive task to confirm a safe send.
    </li><li>A blocking send can be asynchronous if a system buffer is used to
        hold the data for eventual delivery to the receive.
    </li><li>A blocking receive only "returns" after the data has arrived and
        is ready for use by the program.
    </li></ul>
<p>
</p></li><li><b>Non-blocking:</b>
    <ul>
    <li>Non-blocking send and receive routines behave similarly - they will
        return almost immediately. They do not wait for any communication
        events to complete, such as message copying from user memory to 
        system buffer space or the actual arrival of message. 
    </li><li>Non-blocking operations simply "request" the MPI library to perform
        the operation when it is able.  The user can not predict when that
        will happen. 
    </li><li>It is unsafe to modify the application buffer (your
        variable space) until you know for a fact the requested 
        non-blocking operation was actually performed by the library.
        There are "wait" routines used to do this.
    </li><li>Non-blocking communications are primarily used to overlap computation
        with communication and exploit possible performance gains.
<p>
<table border="1" cellpadding="5" cellspacing="0">
<tbody><tr valign="top">
<th>Blocking Send</th>
<th>Non-blocking Send</th>
</tr><tr valign="top">
<td width="50%"><pre>myvar = 0;

for (i=1; i&lt;ntasks; i++) {
   task = i;
   <font color="#DF4442">MPI_Send (&amp;myvar ... ... task ...);
</font>   myvar = myvar + 2

   /* do some work */

   }

</pre></td><td width="50%"><pre>myvar = 0;

for (i=1; i&lt;ntasks; i++) {
   task = i;
   <font color="#DF4442">MPI_Isend (&amp;myvar ... ... task ...);</font>
   myvar = myvar + 2;

   /* do some work */

   <font color="#DF4442">MPI_Wait (...);</font>
   }

</pre></td>
</tr><tr valign="top">
<td align="center"><b>Safe.  Why?<b></b></b></td>
<td align="center"><b>Unsafe. Why?<b></b></b></td>
</tr></tbody></table>
    </p></li></ul>
</li></ul>
<p>

<img src="./Message Passing Interface (MPI)_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Order and Fairness:</span>
</p><ul>
<li><b>Order:</b>
    <ul>
    <li>MPI guarantees that messages will not overtake each other.
    </li><li>If a sender sends two messages (Message 1 and Message 2) in succession 
        to the same destination, and both match the same receive, the receive
        operation will receive Message 1 before Message 2.
    </li><li>If a receiver posts two receives (Receive 1 and Receive 2), in 
        succession, and both are looking for the same message, Receive 1 will
        receive the message before Receive 2.
    </li><li>Order rules do not apply if there are multiple threads participating
        in the communication operations. 
    </li></ul>
<p>
</p></li><li><b>Fairness:</b>
    <ul>
    <li>MPI does not guarantee fairness - it's up to the programmer to
        prevent "operation starvation".
    </li><li>Example: task 0 sends a message to task 2. However, task 1 sends
        a competing message that matches task 2's receive. Only one of the
        sends will complete.
    <p>
    <img src="./Message Passing Interface (MPI)_files/fairness.gif" width="400" height="292" border="0">
    </p></li></ul>
</li></ul>

<!--------------------------------------------------------------------------->

<a name="Routine_Arguments"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Point to Point Communication Routines</span></td>
</tr></tbody></table>
<h2>MPI Message Passing Routine Arguments</h2>

MPI point-to-point communication routines generally have an argument list 
that takes one of the following formats:
<p>
</p><ul>
<table border="1" cellspacing="0" cellpadding="5" width="90%">
<tbody><tr><td bgcolor="#FOF5FE"><b>Blocking sends
    </b></td><td><tt><b><nobr>
    MPI_Send(buffer,count,type,dest,tag,comm) 
</nobr></b></tt></td></tr><tr><td bgcolor="#FOF5FE"><b>Non-blocking sends
    </b></td><td><tt><b><nobr>
    MPI_Isend(buffer,count,type,dest,tag,comm,request) 
</nobr></b></tt></td></tr><tr><td bgcolor="#FOF5FE"><b>Blocking receive
    </b></td><td><tt><b><nobr>
    MPI_Recv(buffer,count,type,source,tag,comm,status) 
</nobr></b></tt></td></tr><tr><td bgcolor="#FOF5FE"><b>Non-blocking receive
    </b></td><td><tt><b><nobr>
    MPI_Irecv(buffer,count,type,source,tag,comm,request) </nobr></b></tt></td>
</tr></tbody></table>
</ul>

<dl>
<dt><b>Buffer</b>
<p>
</p></dt><dd>Program (application) address space that references the data that is 
    to be sent or received.  In most cases, this is simply the variable
    name that is be sent/received.  For C programs, this argument is 
    passed by reference and usually must be prepended with an ampersand:  
    <tt><b> &amp;var1 </b></tt>
<p>
</p></dd><dt><b>Data Count</b>
<p>
</p></dt><dd>Indicates the number of data elements of a particular type to be sent.  
<p>
</p></dd><dt><b>Data Type</b>
<p>
</p></dt><dd>For reasons of portability, MPI predefines its elementary data types.
    The table below lists those required by the standard. 
<p>
<table border="1" cellspacing="0" cellpadding="5" width="90%">
<tbody><tr><th colspan="2">C Data Types</th>
    <th colspan="2">Fortran Data Types</th>
</tr><tr><td bgcolor="#FOF5FE"><tt><b>MPI_CHAR</b></tt></td>
    <td>char</td>
    <td bgcolor="#FOF5FE"><tt><b>MPI_CHARACTER</b></tt></td>
    <td>character(1)</td>
</tr><tr><td bgcolor="#FOF5FE"><tt><b>MPI_WCHAR</b></tt></td>
    <td>wchar_t - wide character</td>
    <td bgcolor="#FOF5FE">&nbsp;</td>
    <td>&nbsp;</td>
</tr><tr><td bgcolor="#FOF5FE"><tt><b>MPI_SHORT</b></tt></td>
    <td>signed short int</td>
    <td bgcolor="#FOF5FE">&nbsp;</td>
    <td>&nbsp;</td>
</tr><tr><td bgcolor="#FOF5FE"><tt><b>MPI_INT</b></tt></td>
    <td>signed int</td>
    <td bgcolor="#FOF5FE"><tt><b>MPI_INTEGER<br><font color="gray">MPI_INTEGER1
    <br>MPI_INTEGER2<br>MPI_INTEGER4</font></b></tt></td>
    <td>integer<br>integer*1<br>integer*2<br>integer*4</td>
</tr><tr><td bgcolor="#FOF5FE"><tt><b>MPI_LONG</b></tt></td>
    <td>signed long int</td>
    <td bgcolor="#FOF5FE">&nbsp;</td>
    <td>&nbsp;</td>
</tr><tr><td bgcolor="#FOF5FE"><tt><b>MPI_LONG_LONG_INT
    <br>MPI_LONG_LONG</b></tt></td>
    <td>signed long long int</td>
    <td bgcolor="#FOF5FE">&nbsp;</td>
    <td>&nbsp;</td>
</tr><tr><td bgcolor="#FOF5FE"><tt><b>MPI_SIGNED_CHAR</b></tt></td>
    <td>signed char</td>
    <td bgcolor="#FOF5FE">&nbsp;</td>
    <td>&nbsp;</td>
</tr><tr><td bgcolor="#FOF5FE"><tt><b>MPI_UNSIGNED_CHAR</b></tt></td>
    <td>unsigned char</td>
    <td bgcolor="#FOF5FE">&nbsp;</td>
    <td>&nbsp;</td>
</tr><tr><td bgcolor="#FOF5FE"><tt><b>MPI_UNSIGNED_SHORT</b></tt></td>
    <td>unsigned short int</td>
    <td bgcolor="#FOF5FE">&nbsp;</td>
    <td>&nbsp;</td>
</tr><tr><td bgcolor="#FOF5FE"><tt><b>MPI_UNSIGNED</b></tt></td>
    <td>unsigned int</td>
    <td bgcolor="#FOF5FE">&nbsp;</td>
    <td>&nbsp;</td>
</tr><tr><td bgcolor="#FOF5FE"><tt><b>MPI_UNSIGNED_LONG</b></tt></td>
    <td>unsigned long int</td>
    <td bgcolor="#FOF5FE">&nbsp;</td>
    <td>&nbsp;</td>
</tr><tr><td bgcolor="#FOF5FE"><tt><b>MPI_UNSIGNED_LONG_LONG</b></tt></td>
    <td>unsigned long long int</td>
    <td bgcolor="#FOF5FE">&nbsp;</td>
    <td>&nbsp;</td>
</tr><tr><td bgcolor="#FOF5FE"><tt><b>MPI_FLOAT</b></tt></td>
    <td>float</td>
    <td bgcolor="#FOF5FE"><tt><b>MPI_REAL<br><font color="gray">MPI_REAL2
    <br>MPI_REAL4<br>MPI_REAL8</font>
    </b></tt></td><td>real<br>real*2<br>real*4<br>real*8</td>
</tr><tr><td bgcolor="#FOF5FE"><tt><b>MPI_DOUBLE</b></tt></td>
    <td>double</td>
    <td bgcolor="#FOF5FE"><tt><b>MPI_DOUBLE_PRECISION</b></tt></td>
    <td>double precision</td>
</tr><tr><td bgcolor="#FOF5FE"><tt><b>MPI_LONG_DOUBLE</b></tt></td>
    <td>long double</td>
    <td bgcolor="#FOF5FE">&nbsp;</td>
    <td>&nbsp;</td>
</tr><tr><td bgcolor="#FOF5FE"><tt><b>MPI_C_COMPLEX<br>MPI_C_FLOAT_COMPLEX</b></tt></td>
    <td>float _Complex</td>
    <td bgcolor="#FOF5FE"><tt><b>MPI_COMPLEX</b></tt></td>
    <td>complex</td>
</tr><tr><td bgcolor="#FOF5FE"><tt><b>MPI_C_DOUBLE_COMPLEX</b></tt></td>
    <td>double _Complex</td>
    <td bgcolor="#FOF5FE"><tt><b><font color="gray">MPI_DOUBLE_COMPLEX</font></b></tt></td>
    <td>double complex</td>
</tr><tr><td bgcolor="#FOF5FE"><tt><b>MPI_C_LONG_DOUBLE_COMPLEX</b></tt></td>
    <td>long double _Complex</td>
    <td bgcolor="#FOF5FE">&nbsp;</td>
    <td>&nbsp;</td>
</tr><tr><td bgcolor="#FOF5FE"><tt><b>MPI_C_BOOL</b></tt></td>
    <td>_Bool</td>
    <td bgcolor="#FOF5FE"><tt><b>MPI_LOGICAL</b></tt></td>
    <td>logical</td>

</tr><tr><td bgcolor="#FOF5FE"><tt><b>MPI_INT8_T 
<br>MPI_INT16_T<br>MPI_INT32_T <br>MPI_INT64_T</b></tt></td>
    <td>int8_t<br>int16_t<br>int32_t <br>int64_t</td>
    <td bgcolor="#FOF5FE">&nbsp;</td>
    <td>&nbsp;</td>
</tr><tr><td bgcolor="#FOF5FE"><tt><b>MPI_UINT8_T 
<br>MPI_UINT16_T <br>MPI_UINT32_T <br>MPI_UINT64_T </b></tt></td>
    <td>uint8_t<br>uint16_t<br>uint32_t<br>uint64_t</td>
    <td bgcolor="#FOF5FE">&nbsp;</td>
    <td>&nbsp;</td>
</tr><tr><td bgcolor="#FOF5FE"><tt><b>MPI_BYTE</b></tt></td>
    <td>8 binary digits </td>
    <td bgcolor="#FOF5FE"><tt><b>MPI_BYTE</b></tt></td>        
    <td>8 binary digits </td>
</tr><tr><td bgcolor="#FOF5FE"><tt><b>MPI_PACKED</b></tt></td>
    <td>data packed or unpacked with MPI_Pack()/
        MPI_Unpack</td>
    <td bgcolor="#FOF5FE"><tt><b>MPI_PACKED</b></tt></td>
    <td>data packed or unpacked with MPI_Pack()/
        MPI_Unpack</td>
</tr></tbody></table>
</p><p>
<b>Notes:</b>
    </p><ul>
    <li>Programmers may also create their own data types 
        (see <a href="https://computing.llnl.gov/tutorials/mpi/#Derived_Data_Types">Derived Data Types</a>).
    </li><li>MPI_BYTE and MPI_PACKED do not correspond to standard C or
        Fortran types.
    </li><li>Types shown in <font color="gray"><b>GRAY FONT</b></font> are recommended if
        possible.  
    </li><li>Some implementations may include additional elementary data
        types (MPI_LOGICAL2, MPI_COMPLEX32, etc.). Check the MPI header file.
    </li></ul>
<p>
</p></dd><dt><b>Destination</b>
<p>
</p></dt><dd>An argument to send routines that indicates the process where a 
    message should be delivered.  Specified as the rank of the receiving 
    process.
<p>
</p></dd><dt><b>Source</b>
<p>
</p></dt><dd>An argument to receive routines that indicates the originating process
    of the message.  Specified as the rank of the sending process.  This may 
    be set to the wild card MPI_ANY_SOURCE to receive a message from any task.
<p>
</p></dd><dt><b>Tag</b>
<p>
</p></dt><dd>Arbitrary non-negative integer assigned by the programmer to uniquely 
    identify a message.  Send and receive operations should match message 
    tags.  For a receive operation, the wild card MPI_ANY_TAG can be used 
    to receive any message regardless of its tag.  The MPI standard guarantees 
    that integers 0-32767 can be used as tags, but most implementations allow 
    a much larger range than this.
<p>
</p></dd><dt><b>Communicator</b>
<p>
</p></dt><dd>Indicates the communication context, or set of processes for which the
    source or destination fields are valid.  Unless the programmer is 
    explicitly creating new communicators, the predefined communicator
    MPI_COMM_WORLD is usually used.
<p>
</p></dd><dt><b>Status</b>
<p>
</p></dt><dd>For a receive operation, indicates the source of the message and the 
    tag of the message.  In C, this argument is a pointer to a predefined 
    structure MPI_Status (ex. <tt>stat.MPI_SOURCE stat.MPI_TAG</tt>).
    In Fortran, it is an integer array of size MPI_STATUS_SIZE (ex.
    <tt>stat(MPI_SOURCE) stat(MPI_TAG)</tt>).  Additionally, the actual number of
    bytes received is obtainable from Status via the MPI_Get_count
    routine. The constants MPI_STATUS_IGNORE and MPI_STATUSES_IGNORE can be 
    substituted if a message's source, tag or size will be be queried later.
<p>
</p></dd><dt><b>Request</b>
<p>
</p></dt><dd>Used by non-blocking send and receive operations.  Since non-blocking
    operations may return before the requested system buffer space is
    obtained, the system issues a unique "request number".  The programmer
    uses this system assigned "handle" later (in a WAIT type routine)
    to determine completion of the non-blocking operation.  In C, this 
    argument is a pointer to a predefined structure MPI_Request. In Fortran, 
    it is an integer. 
</dd></dl>

<!--------------------------------------------------------------------------->

<a name="Blocking_Message_Passing_Routines"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Point to Point Communication Routines</span></td>
</tr></tbody></table>
<h2>Blocking Message Passing Routines</h2>

The more commonly used MPI blocking message passing routines are described
below.
<p>
<b><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Send.txt" target="_blank"> 
MPI_Send</a></b>
</p><p>
</p><ul>Basic blocking send operation.  Routine returns only after the 
    application buffer in the sending task is free for reuse.  Note that
    this routine may be implemented differently on different systems.  The 
    MPI standard permits the use of a system buffer but does not require it.  
    Some implementations may actually use a synchronous send (discussed
    below) to implement the basic blocking send. 
<p>
<table border="1" cellpadding="5" cellspacing="0" width="75%">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Send (&amp;buf,count,datatype,dest,tag,comm)  <br>
    MPI_SEND (buf,count,datatype,dest,tag,comm,ierr)
</b></tt></nobr></td></tr><tr></tr></tbody></table>
</p></ul><p><br>

<b><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Recv.txt" target="_blank"> 
MPI_Recv</a></b> 
</p><p>
</p><ul>Receive a message and block until the requested data is available in 
    the application buffer in the receiving task.
<p>
<table border="1" cellpadding="5" cellspacing="0" width="75%">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Recv (&amp;buf,count,datatype,source,tag,comm,&amp;status) <br> 
    MPI_RECV (buf,count,datatype,source,tag,comm,status,ierr) 
</b></tt></nobr></td></tr><tr></tr></tbody></table>
</p></ul><p><br>

<b><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Ssend.txt" target="_blank"> 
MPI_Ssend</a></b> 
</p><p>
</p><ul>Synchronous blocking send: Send a message and block until the
    application buffer in the sending task is free for reuse and the
    destination process has started to receive the message.
<p>
<table border="1" cellpadding="5" cellspacing="0" width="75%">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Ssend (&amp;buf,count,datatype,dest,tag,comm)  <br>
    MPI_SSEND (buf,count,datatype,dest,tag,comm,ierr)
</b></tt></nobr></td></tr><tr></tr></tbody></table>
</p></ul><p><br>

<!------------------------ removed 6/16 -----------------------------------
<B><A HREF=man/MPI_Bsend.txt TARGET=_blank> 
MPI_Bsend</A></B> 
<P>
<UL>Buffered blocking send: permits the programmer to allocate the required
    amount of buffer space into which data can be copied until it is 
    delivered.  Insulates against the problems associated with insufficient
    system buffer space.  Routine returns after the data has been copied
    from application buffer space to the allocated send buffer.  Must be
    used with the MPI_Buffer_attach routine.
<P>
<TABLE BORDER=1 CELLPADDING=5 CELLSPACING=0 WIDTH=75%>
<TR VALIGN=top><TD><NOBR><TT><B> 
    MPI_Bsend (&amp;buf,count,datatype,dest,tag,comm)  <BR>
    MPI_BSEND (buf,count,datatype,dest,tag,comm,ierr)
</B></TT></NOBR></TD><TR></TABLE>
</UL><P><BR>

<B><A HREF=man/MPI_Buffer_attach.txt TARGET=_blank> 
MPI_Buffer_attach</A> <BR> 
<A HREF=man/MPI_Buffer_detach.txt TARGET=_blank> 
MPI_Buffer_detach</A></B>
<P>
<UL>Used by programmer to allocate/deallocate message buffer space to be 
    used by the MPI_Bsend routine.  The size argument is specified in 
    actual data bytes - not a count of data elements.
    Only one buffer can be attached to a process at a time.
<P>
<TABLE BORDER=1 CELLPADDING=5 CELLSPACING=0 WIDTH=75%>
<TR VALIGN=top><TD><NOBR><TT><B> 
    MPI_Buffer_attach (&amp;buffer,size)  <BR>
    MPI_Buffer_detach (&amp;buffer,size)  <BR>
    MPI_BUFFER_ATTACH (buffer,size,ierr)  <BR>
    MPI_BUFFER_DETACH (buffer,size,ierr)
</B></TT></NOBR></TD><TR></TABLE>
</UL><P><BR>

<B><A HREF=man/MPI_Rsend.txt TARGET=_blank> 
MPI_Rsend</A></B> 
<P>
<UL>Blocking ready send.  Should only be used if the programmer is certain 
    that the matching receive has already been posted. Often simply implemented as
    an MPI_Send routine.
<P>
<TABLE BORDER=1 CELLPADDING=5 CELLSPACING=0 WIDTH=75%>
<TR VALIGN=top><TD><NOBR><TT><B> 
    MPI_Rsend (&amp;buf,count,datatype,dest,tag,comm) <BR> 
    MPI_RSEND (buf,count,datatype,dest,tag,comm,ierr)
</B></TT></NOBR></TD><TR></TABLE>
</UL><P><BR>

------------------------ removed 6/16 ----------------------------------->

<b><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Sendrecv.txt" target="_blank"> 
MPI_Sendrecv</a></b> 
</p><p>
</p><ul>Send a message and post a receive before blocking.  Will block until
    the sending application buffer is free for reuse and until the receiving
    application buffer contains the received message.
<p>
<table border="1" cellpadding="5" cellspacing="0" width="75%">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Sendrecv (&amp;sendbuf,sendcount,sendtype,dest,sendtag,  <br>
        <font color="#FFFFFF">......</font> 
                 &amp;recvbuf,recvcount,recvtype,source,recvtag,   <br>
        <font color="#FFFFFF">......</font> 
                 comm,&amp;status)   <br>
    MPI_SENDRECV (sendbuf,sendcount,sendtype,dest,sendtag,  <br> 
        <font color="#FFFFFF">......</font> 
                 recvbuf,recvcount,recvtype,source,recvtag,  <br>
        <font color="#FFFFFF">......</font> 
                 comm,status,ierr)
    </b></tt></nobr><p>
</p></td></tr><tr></tr></tbody></table>
</p></ul><p><br>

<b><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Wait.txt" target="_blank">MPI_Wait</a><br>
<a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Waitany.txt" target="_blank">MPI_Waitany</a><br>
<a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Waitall.txt" target="_blank">MPI_Waitall</a><br>
<a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Waitsome.txt" target="_blank"> MPI_Waitsome</a></b>
</p><p>
</p><ul>MPI_Wait blocks until a specified non-blocking send or receive
    operation has completed.  For multiple non-blocking operations, the
    programmer can specify any, all or some completions.
<p>
<table border="1" cellpadding="5" cellspacing="0" width="75%">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Wait     (&amp;request,&amp;status)  <br>
    MPI_Waitany  (count,&amp;array_of_requests,&amp;index,&amp;status)  <br>
    MPI_Waitall  (count,&amp;array_of_requests,&amp;array_of_statuses)  <br>
    MPI_Waitsome (incount,&amp;array_of_requests,&amp;outcount,  <br>
        <font color="#FFFFFF">......</font> 
        &amp;array_of_offsets, &amp;array_of_statuses)  <br>
    MPI_WAIT     (request,status,ierr)  <br>
    MPI_WAITANY  (count,array_of_requests,index,status,ierr)  <br>
    MPI_WAITALL  (count,array_of_requests,array_of_statuses,  <br>
        <font color="#FFFFFF">......</font> 
                 ierr)  <br>
    MPI_WAITSOME (incount,array_of_requests,outcount,  <br>
        <font color="#FFFFFF">......</font> 
                 array_of_offsets, array_of_statuses,ierr)
</b></tt></nobr></td></tr><tr></tr></tbody></table>
</p></ul><p><br>

<b><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Probe.txt" target="_blank"> 
MPI_Probe</a></b>
</p><p>
</p><ul>Performs a blocking test for a message. The "wildcards"  MPI_ANY_SOURCE 
    and MPI_ANY_TAG may be used to test for a message from any source or
    with any tag.  For the C routine, the actual source and tag will be
    returned in the status structure as <tt>status.MPI_SOURCE</tt> and
    <tt>status.MPI_TAG</tt>.  For the Fortran routine, they will be returned in
    the integer array <tt>status(MPI_SOURCE)</tt> and <tt>status(MPI_TAG)</tt>.
    <input type="button" value="Example Use" onclick="popUp(&#39;samples/mpi_probe.txt&#39;)">
<p>
<table border="1" cellpadding="5" cellspacing="0" width="75%">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Probe (source,tag,comm,&amp;status)  <br>
    MPI_PROBE (source,tag,comm,status,ierr)
</b></tt></nobr></td></tr><tr></tr></tbody></table>
</p></ul><p><br>

<b><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Get_count.txt" target="_blank"> 
MPI_Get_count</a></b>
</p><p>
</p><ul>Returns the source, tag and number of elements of datatype received. Can be
    used with both blocking and non-blocking receive operations.
    For the C routine, the actual source and tag will be
    returned in the status structure as <tt>status.MPI_SOURCE</tt> and
    <tt>status.MPI_TAG</tt>.  For the Fortran routine, they will be returned in
    the integer array <tt>status(MPI_SOURCE)</tt> and <tt>status(MPI_TAG)</tt>.
<p>
<table border="1" cellpadding="5" cellspacing="0" width="75%">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Get_count (&amp;status,datatype,&amp;count)  <br>
    MPI_GET_COUNT (status,datatype,count,ierr)
</b></tt></nobr></td></tr><tr></tr></tbody></table>
</p></ul><p><br>





</p><p></p><hr><p>

</p><h2>Examples: Blocking Message Passing Routines</h2>

<ul>
<p>
Task 0 pings task 1 and awaits return ping
</p><p>

<table border="1" cellpadding="15" cellspacing="0" width="90%"><tbody><tr><td> <!---outer table--->
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tbody><tr>
<td width="30" bgcolor="FOF5FE" colspan="2" align="center"><img src="./Message Passing Interface (MPI)_files/page01.gif"></td>
<td bgcolor="FOF5FE"><b><br>&nbsp;&nbsp;&nbsp;&nbsp;
C Language - Blocking Message Passing Example
</b><p></p></td>

</tr><tr valign="top"><td colspan="3"><p></p></td>

</tr><tr valign="top">
<td width="30"><pre><font color="#AAAAAA"> 1<br> 2<br> 3<br> 4<br> 5<br> 6<br> 7<br> 8<br> 9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35</font></pre></td>

<td width="1" bgcolor="#7099cc"></td>

<td><pre>   #include <font color="#df4442">"mpi.h"</font>
   #include &lt;stdio.h&gt;

   main(int argc, char *argv[])  {
   int numtasks, rank, dest, source, rc, count, tag=1;  
   char inmsg, outmsg='x';
   <font color="#DF4442">MPI_Status Stat</font>;   <font color="#AAAAAA">// required variable for receive routines</font>

   <font color="#DF4442">MPI_Init</font>(&amp;argc,&amp;argv);
   <font color="#DF4442">MPI_Comm_size</font>(MPI_COMM_WORLD, &amp;numtasks);
   <font color="#DF4442">MPI_Comm_rank</font>(MPI_COMM_WORLD, &amp;rank);

   <font color="#AAAAAA">// task 0 sends to task 1 and waits to receive a return message</font>
   if (rank == 0) {
     dest = 1;
     source = 1;
     <font color="#DF4442">MPI_Send</font>(&amp;outmsg, 1, MPI_CHAR, dest, tag, MPI_COMM_WORLD);
     <font color="#DF4442">MPI_Recv</font>(&amp;inmsg, 1, MPI_CHAR, source, tag, MPI_COMM_WORLD, &amp;Stat);
     } 

   <font color="#AAAAAA">// task 1 waits for task 0 message then returns a message</font>
   else if (rank == 1) {
     dest = 0;
     source = 0;
     <font color="#DF4442">MPI_Recv</font>(&amp;inmsg, 1, MPI_CHAR, source, tag, MPI_COMM_WORLD, &amp;Stat);
     <font color="#DF4442">MPI_Send</font>(&amp;outmsg, 1, MPI_CHAR, dest, tag, MPI_COMM_WORLD);
     }

   <font color="#AAAAAA">// query receive Stat variable and print message details</font>
   <font color="#DF4442">MPI_Get_count</font>(&amp;Stat, MPI_CHAR, &amp;count);
   printf("Task %d: Received %d char(s) from task %d with tag %d \n",
          rank, count, Stat.MPI_SOURCE, Stat.MPI_TAG);

   <font color="#DF4442">MPI_Finalize</font>();
   }

</pre></td>
</tr></tbody></table>
</td></tr></tbody></table>  <!---outer table--->

<br><br><br>

<table border="1" cellpadding="15" cellspacing="0" width="90%"><tbody><tr><td> <!---outer table--->
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tbody><tr>
<td width="30" bgcolor="FOF5FE" colspan="2" align="center"><img src="./Message Passing Interface (MPI)_files/page01.gif"></td>
<td bgcolor="FOF5FE"><b><br>&nbsp;&nbsp;&nbsp;&nbsp;
Fortran - Blocking Message Passing Example
</b><p></p></td>

</tr><tr valign="top"><td colspan="3"><p></p></td>

</tr><tr valign="top">
<td width="30"><pre><font color="#AAAAAA"> 1<br> 2<br> 3<br> 4<br> 5<br> 6<br> 7<br> 8<br> 9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36</font></pre></td>

<td width="1" bgcolor="#7099cc"></td>

<td><pre>   program ping
   include <font color="#DF4442">'mpif.h'</font>

   integer numtasks, rank, dest, source, count, tag, ierr
   integer <font color="#DF4442">stat(MPI_STATUS_SIZE)</font>   <font color="#AAAAAA">! required variable for receive routines</font>
   character inmsg, outmsg
   outmsg = 'x'
   tag = 1

   call <font color="#DF4442">MPI_INIT</font>(ierr)
   call <font color="#DF4442">MPI_COMM_RANK</font>(MPI_COMM_WORLD, rank, ierr)
   call <font color="#DF4442">MPI_COMM_SIZE</font>(MPI_COMM_WORLD, numtasks, ierr)

   <font color="#AAAAAA">! task 0 sends to task 1 and waits to receive a return message</font>
   if (rank .eq. 0) then
      dest = 1
      source = 1
      call <font color="#DF4442">MPI_SEND</font>(outmsg, 1, MPI_CHARACTER, dest, tag, MPI_COMM_WORLD, ierr)
      call <font color="#DF4442">MPI_RECV</font>(inmsg, 1, MPI_CHARACTER, source, tag, MPI_COMM_WORLD, stat, ierr)

   <font color="#AAAAAA">! task 1 waits for task 0 message then returns a message</font>
   else if (rank .eq. 1) then
      dest = 0
      source = 0
      call <font color="#DF4442">MPI_RECV</font>(inmsg, 1, MPI_CHARACTER, source, tag, MPI_COMM_WORLD, stat, err)
      call <font color="#DF4442">MPI_SEND</font>(outmsg, 1, MPI_CHARACTER, dest, tag, MPI_COMM_WORLD, err)
   endif

   <font color="#AAAAAA">! query receive Stat variable and print message details</font>
   call <font color="#DF4442">MPI_GET_COUNT</font>(stat, MPI_CHARACTER, count, ierr)
   print *, 'Task ',rank,': Received', count, 'char(s) from task', &amp;
            stat(MPI_SOURCE), 'with tag',stat(MPI_TAG)

   call <font color="#DF4442">MPI_FINALIZE</font>(ierr)

   end

</pre></td>
</tr></tbody></table>
</td></tr></tbody></table>  <!---outer table--->
</p></ul>

<!--------------------------------------------------------------------------->

<a name="Non-Blocking_Message_Passing_Routines"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Point to Point Communication Routines</span></td>
</tr></tbody></table>
<h2>Non-blocking Message Passing Routines</h2>

The more commonly used MPI non-blocking message passing routines are described
below.
<p>
<b><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Isend.txt" target="_blank"> 
MPI_Isend</a></b> 
</p><p>
</p><ul>Identifies an area in memory to serve as a send buffer.  Processing 
    continues immediately without waiting for the message to be copied out 
    from the application buffer.  A communication request handle is 
    returned for handling the pending message status.  The program should 
    not modify the application buffer until subsequent calls to MPI_Wait
    or MPI_Test indicate that the non-blocking send has completed. 
<p>
<table border="1" cellpadding="5" cellspacing="0" width="75%">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Isend (&amp;buf,count,datatype,dest,tag,comm,&amp;request) <br>
    MPI_ISEND (buf,count,datatype,dest,tag,comm,request,ierr)
</b></tt></nobr></td></tr><tr></tr></tbody></table>
</p></ul><p><br>

<b><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Irecv.txt" target="_blank"> 
MPI_Irecv</a></b>
</p><p>
</p><ul>Identifies an area in memory to serve as a receive buffer.  Processing
    continues immediately without actually waiting for the message to be
    received and copied into the the application buffer.  A communication
    request handle is returned for handling the pending message status.
    The program must use calls to MPI_Wait or MPI_Test to determine when the
    non-blocking receive operation completes and the requested message is
    available in the application buffer.
<p>
<table border="1" cellpadding="5" cellspacing="0" width="75%">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Irecv (&amp;buf,count,datatype,source,tag,comm,&amp;request) <br>
    MPI_IRECV (buf,count,datatype,source,tag,comm,request,ierr)
</b></tt></nobr></td></tr><tr></tr></tbody></table>
</p></ul><p><br>

<b><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Issend.txt" target="_blank"> 
MPI_Issend</a></b> 
</p><p>
</p><ul>Non-blocking synchronous send.  Similar to MPI_Isend(), except 
    MPI_Wait() or MPI_Test() indicates when the destination process has 
    received the message.
<p>
<table border="1" cellpadding="5" cellspacing="0" width="75%">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Issend (&amp;buf,count,datatype,dest,tag,comm,&amp;request) <br>
    MPI_ISSEND (buf,count,datatype,dest,tag,comm,request,ierr)
</b></tt></nobr></td></tr><tr></tr></tbody></table>
</p></ul><p><br>

<!------------------------ removed 6/16 -----------------------------------
<B><A HREF=man/MPI_Ibsend.txt TARGET=_blank> 
MPI_Ibsend</A></B> 
<P>
<UL>Non-blocking buffered send.  Similar to MPI_Bsend() except MPI_Wait() 
    or MPI_Test() indicates when the destination process has received the 
    message.  Must be used with the MPI_Buffer_attach routine.
<P>
<TABLE BORDER=1 CELLPADDING=5 CELLSPACING=0 WIDTH=75%>
<TR VALIGN=top><TD><NOBR><TT><B> 
    MPI_Ibsend (&amp;buf,count,datatype,dest,tag,comm,&amp;request) <BR>
    MPI_IBSEND (buf,count,datatype,dest,tag,comm,request,ierr)
</B></TT></NOBR></TD><TR></TABLE>
</UL><P><BR>

<B><A HREF=man/MPI_Irsend.txt TARGET=_blank> 
MPI_Irsend</A></B>
<P>
<UL>Non-blocking ready send.  Similar to MPI_Rsend() except MPI_Wait()
    or MPI_Test() indicates when the destination process has received the 
    message.  Should only be used if the programmer is certain that the 
    matching receive has already been posted.
<P>
<TABLE BORDER=1 CELLPADDING=5 CELLSPACING=0 WIDTH=75%>
<TR VALIGN=top><TD><NOBR><TT><B> 
    MPI_Irsend (&amp;buf,count,datatype,dest,tag,comm,&amp;request) <BR>
    MPI_IRSEND (buf,count,datatype,dest,tag,comm,request,ierr)
</B></TT></NOBR></TD><TR></TABLE>
</UL><P><BR>
------------------------ removed 6/16 ----------------------------------->

<b><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Test.txt" target="_blank"> MPI_Test</a> 
<br><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Testany.txt" target="_blank"> MPI_Testany</a> 
<br><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Testall.txt" target="_blank"> MPI_Testall</a> 
<br><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Testsome.txt" target="_blank"> MPI_Testsome</a>
</b>
</p><p>
</p><ul>MPI_Test checks the status of a specified non-blocking send or receive 
    operation. The "flag" parameter is returned logical true (1) if the
    operation has completed, and logical false (0) if not.  For multiple
    non-blocking operations, the programmer can specify any, all or some 
    completions.
<p>
<table border="1" cellpadding="5" cellspacing="0" width="75%">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Test     (&amp;request,&amp;flag,&amp;status) <br>
    MPI_Testany  (count,&amp;array_of_requests,&amp;index,&amp;flag,&amp;status)<br>
    MPI_Testall  (count,&amp;array_of_requests,&amp;flag,&amp;array_of_statuses)<br>
    MPI_Testsome (incount,&amp;array_of_requests,&amp;outcount,<br>
        <font color="#FFFFFF">......</font> 
                 &amp;array_of_offsets, &amp;array_of_statuses)<br>
    MPI_TEST     (request,flag,status,ierr)<br>
    MPI_TESTANY  (count,array_of_requests,index,flag,status,ierr)<br>
    MPI_TESTALL  (count,array_of_requests,flag,array_of_statuses,ierr)<br>
    MPI_TESTSOME (incount,array_of_requests,outcount,<br>
        <font color="#FFFFFF">......</font> 
                 array_of_offsets, array_of_statuses,ierr)<br>
</b></tt></nobr></td></tr><tr></tr></tbody></table>
</p></ul><p><br>

<b><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Iprobe.txt" target="_blank"> 
MPI_Iprobe</a></b>
</p><p>
</p><ul>Performs a non-blocking test for a message. The "wildcards"  
    MPI_ANY_SOURCE and MPI_ANY_TAG may be used to test for a message 
    from any source or with any tag.  The integer "flag" parameter is returned
    logical true (1) if a message has arrived, and logical false (0) if not.
    For the C routine, the actual source and tag will be
    returned in the status structure as <tt>status.MPI_SOURCE</tt> and
    <tt>status.MPI_TAG</tt>.  For the Fortran routine, they will be returned in
    the integer array <tt>status(MPI_SOURCE)</tt> and <tt>status(MPI_TAG)</tt>.
<p>
<table border="1" cellpadding="5" cellspacing="0" width="75%">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Iprobe (source,tag,comm,&amp;flag,&amp;status)<br>
    MPI_IPROBE (source,tag,comm,flag,status,ierr)
</b></tt></nobr></td></tr><tr></tr></tbody></table>
</p></ul><p><br>

</p><p></p><hr><p>

</p><h2>Examples: Non-blocking Message Passing Routines</h2>

<ul>
<p>
Nearest neighbor exchange in a ring topology 
</p><p>
<img src="./Message Passing Interface (MPI)_files/ringtopo.gif" width="583" height="79" border="0">
</p><p><br>

<table border="1" cellpadding="15" cellspacing="0" width="90%"><tbody><tr><td> <!---outer table--->
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tbody><tr>
<td width="30" bgcolor="FOF5FE" colspan="2" align="center"><img src="./Message Passing Interface (MPI)_files/page01.gif"></td>
<td bgcolor="FOF5FE"><b><br>&nbsp;&nbsp;&nbsp;&nbsp;
C Language - Non-blocking Message Passing Example</b><p></p></td>

</tr><tr valign="top"><td colspan="3"><p></p></td>

</tr><tr valign="top">
<td width="30"><pre><font color="#AAAAAA"> 1<br> 2<br> 3<br> 4<br> 5<br> 6<br> 7<br> 8<br> 9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34</font></pre></td>

<td width="1" bgcolor="#7099cc"></td>

<td><pre>   #include <font color="#DF4442">"mpi.h"</font>
   #include &lt;stdio.h&gt;

   main(int argc, char *argv[])  {
   int numtasks, rank, next, prev, buf[2], tag1=1, tag2=2;
   <font color="#DF4442">MPI_Request reqs[4]</font>;   <font color="#AAAAAA">// required variable for non-blocking calls</font>
   <font color="#DF4442">MPI_Status stats[4]</font>;   <font color="#AAAAAA">// required variable for Waitall routine</font>

   <font color="#DF4442">MPI_Init</font>(&amp;argc,&amp;argv);
   <font color="#DF4442">MPI_Comm_size</font>(MPI_COMM_WORLD, &amp;numtasks);
   <font color="#DF4442">MPI_Comm_rank</font>(MPI_COMM_WORLD, &amp;rank);
   
   <font color="#AAAAAA">// determine left and right neighbors</font>
   prev = rank-1;
   next = rank+1;
   if (rank == 0)  prev = numtasks - 1;
   if (rank == (numtasks - 1))  next = 0;

   <font color="#AAAAAA">// post non-blocking receives and sends for neighbors</font>
   <font color="#DF4442">MPI_Irecv</font>(&amp;buf[0], 1, MPI_INT, prev, tag1, MPI_COMM_WORLD, &amp;reqs[0]);
   <font color="#DF4442">MPI_Irecv</font>(&amp;buf[1], 1, MPI_INT, next, tag2, MPI_COMM_WORLD, &amp;reqs[1]);

   <font color="#DF4442">MPI_Isend</font>(&amp;rank, 1, MPI_INT, prev, tag2, MPI_COMM_WORLD, &amp;reqs[2]);
   <font color="#DF4442">MPI_Isend</font>(&amp;rank, 1, MPI_INT, next, tag1, MPI_COMM_WORLD, &amp;reqs[3]);
  
   <font color="#AAAAAA">   // do some work while sends/receives progress in background</font>

   <font color="#AAAAAA">// wait for all non-blocking operations to complete</font>
   <font color="#DF4442">MPI_Waitall</font>(4, reqs, stats);
  
   <font color="#AAAAAA">   // continue - do more work</font>

   <font color="#DF4442">MPI_Finalize</font>();
   }

</pre></td>
</tr></tbody></table>
</td></tr></tbody></table>  <!---outer table--->

<br><br><br>

<table border="1" cellpadding="15" cellspacing="0" width="90%"><tbody><tr><td> <!---outer table--->
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tbody><tr>
<td width="30" bgcolor="FOF5FE" colspan="2" align="center"><img src="./Message Passing Interface (MPI)_files/page01.gif"></td>
<td bgcolor="FOF5FE"><b><br>&nbsp;&nbsp;&nbsp;&nbsp;
Fortran - Non-blocking Message Passing Example
</b><p></p></td>

</tr><tr valign="top"><td colspan="3"><p></p></td>

</tr><tr valign="top">
<td width="30"><pre><font color="#AAAAAA"> 1<br> 2<br> 3<br> 4<br> 5<br> 6<br> 7<br> 8<br> 9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40</font></pre></td>

<td width="1" bgcolor="#7099cc"></td>

<td><pre>   program ringtopo
   include <font color="#DF4442">'mpif.h'</font>

   integer numtasks, rank, next, prev, buf(2), tag1, tag2, ierr
   integer <font color="#DF4442">reqs(4)</font>   <font color="#AAAAAA">! required variable for non-blocking calls </font>
   integer <font color="#DF4442">stats(MPI_STATUS_SIZE,4)</font>   <font color="#AAAAAA">! required variable for WAITALL routine </font>
   tag1 = 1
   tag2 = 2

   call <font color="#DF4442">MPI_INIT</font>(ierr)
   call <font color="#DF4442">MPI_COMM_RANK</font>(MPI_COMM_WORLD, rank, ierr)
   call <font color="#DF4442">MPI_COMM_SIZE</font>(MPI_COMM_WORLD, numtasks, ierr)

   <font color="#AAAAAA">! determine left and right neighbors </font>
   prev = rank - 1
   next = rank + 1
   if (rank .eq. 0) then
      prev = numtasks - 1
   endif
   if (rank .eq. numtasks - 1) then
      next = 0
   endif

   <font color="#AAAAAA">! post non-blocking receives and sends for neighbors </font>
   call <font color="#DF4442">MPI_IRECV</font>(buf(1), 1, MPI_INTEGER, prev, tag1, MPI_COMM_WORLD, reqs(1), ierr)
   call <font color="#DF4442">MPI_IRECV</font>(buf(2), 1, MPI_INTEGER, next, tag2, MPI_COMM_WORLD, reqs(2), ierr)

   call <font color="#DF4442">MPI_ISEND</font>(rank, 1, MPI_INTEGER, prev, tag2, MPI_COMM_WORLD, reqs(3), ierr)
   call <font color="#DF4442">MPI_ISEND</font>(rank, 1, MPI_INTEGER, next, tag1, MPI_COMM_WORLD, reqs(4), ierr)

   <font color="#AAAAAA">   ! do some work while sends/receives progress in background</font>

   <font color="#AAAAAA">! wait for all non-blocking operations to complete </font>
   call <font color="#DF4442">MPI_WAITALL</font>(4, reqs, stats, ierr);

   <font color="#AAAAAA">   ! continue - do more work</font>

   call <font color="#DF4442">MPI_FINALIZE</font>(ierr)

   end

</pre></td>
</tr></tbody></table>
</td></tr></tbody></table>  <!---outer table--->
</p></ul>

<!--------------------------------------------------------------------------->

<a name="Exercise2"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">MPI Exercise 2</span></td>
</tr></tbody></table>
<h2>Point-to-Point Message Passing</h2>

<dd>
<table border="1" cellpadding="5" cellspacing="0" width="90%">
<tbody><tr valign="top"><td bgcolor="#FOF5FE" height="400">
<b><u>Overview:</u>
<ul>
<li>Login to the LC workshop cluster, if you are not already logged in
</li><li>Using your "Hello World" MPI program from Exercise 1, add
    MPI blocking point-to-point routines to send and receive messages
</li><li>Successfully compile your program
</li><li>Successfully run your program - several different ways
</li><li>Try the same thing with nonblocking send/receive routines
</li></ul>
</b><p><b>
<img src="./Message Passing Interface (MPI)_files/point02.jpg" width="100" heigth="45" border="0">
<a href="https://computing.llnl.gov/tutorials/mpi/exercise.html#Exercise2" target="_blank">GO TO THE EXERCISE HERE</a>
</b>
</p><ul>
<h3>Approx. 20 minutes</h3>
</ul>
</td></tr></tbody></table>
</dd>

<!--------------------------------------------------------------------------->

<a name="Collective_Communication_Routines"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Collective Communication Routines</span></td>
</tr></tbody></table>
<p><br>

<img src="./Message Passing Interface (MPI)_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Types of Collective Operations:</span>
<img src="./Message Passing Interface (MPI)_files/collective_comm.gif" width="500" border="0" hspace="20" align="right">
</p><ul>
<p>
</p><li><b>Synchronization</b> - processes wait until all members of the group 
    have reached the synchronization point.
<p>
</p></li><li><b>Data Movement</b> - broadcast, scatter/gather, all to all. 
<p>
</p></li><li><b>Collective Computation</b> (reductions) - one member of the group 
    collects data from the other members and performs an operation 
    (min, max, add, multiply, etc.) on that data. 
</li></ul>
<p>

<img src="./Message Passing Interface (MPI)_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Scope:</span>
</p><ul>
<p>
</p><li>Collective communication routines must involve <b>all</b> processes 
    within the scope of a communicator.  
    <ul>
    <li>All processes are by default, members in the communicator MPI_COMM_WORLD.
    </li><li>Additional communicators can be defined by the programmer. See the
        <a href="https://computing.llnl.gov/tutorials/mpi/#Group_Management_Routines">Group and Communicator Management
        Routines</a> section for details.
    </li></ul>
<p>
</p></li><li>Unexpected behavior, including program failure, can occur if even one
    task in the communicator doesn't participate.
<p>
</p></li><li>It is the programmer's responsibility to ensure that all processes 
    within a communicator participate in any collective operations.
</li></ul>
<p>

<img src="./Message Passing Interface (MPI)_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Programming Considerations and Restrictions:</span>
</p><ul>
<p>
</p><li>Collective communication routines do not take message tag arguments. 
<p>
</p></li><li>Collective operations within subsets of processes are accomplished 
    by first partitioning the subsets into new groups and then
    attaching the new groups to new communicators (discussed in the
    <a href="https://computing.llnl.gov/tutorials/mpi/#Group_Management_Routines">Group and Communicator 
    Management Routines</a> section).
<p>
</p></li><li>Can only be used with MPI predefined datatypes - not with MPI
    <a href="https://computing.llnl.gov/tutorials/mpi/#Derived_Data_Types">Derived Data Types</a>.
<p>
</p></li><li>MPI-2 extended most collective operations to allow data movement
    between intercommunicators (not covered here).
<p>
</p></li><li>With MPI-3, collective operations can be blocking or non-blocking. Only 
    blocking operations are covered in this tutorial. 
</li></ul>

<p></p><hr><p>

</p><h2>Collective Communication Routines</h2>
<p>

<b><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Barrier.txt" target="_blank"> 
MPI_Barrier</a></b>
</p><p>
</p><ul>Synchronization operation. Creates a barrier synchronization in a group.  
    Each task, when reaching the MPI_Barrier call, blocks until all tasks in the 
    group reach the same MPI_Barrier call.  Then all tasks are free to proceed.
<p>
<table border="1" cellpadding="5" cellspacing="0" width="75%">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Barrier (comm)<br>
    MPI_BARRIER (comm,ierr)
</b></tt></nobr></td></tr><tr></tr></tbody></table>
</p></ul><p><br>

<b><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Bcast.txt" target="_blank"> 
MPI_Bcast</a></b>
</p><p>
</p><ul>Data movement operation.
    Broadcasts (sends) a message from the process with rank "root" to all 
    other processes in the group.  
    <br><font size="-1"><b><input type="button" value="Diagram Here" onclick="popUp(&#39;images/MPI_Bcast.gif&#39;)"> </b></font>
    <!--                                                  
    <A HREF="images/MPI_Bcast.gif" TARGET=_blank> Diagram here.</A>  
    -->
<p>
<table border="1" cellpadding="5" cellspacing="0" width="75%">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Bcast (&amp;buffer,count,datatype,root,comm)   <br>
    MPI_BCAST (buffer,count,datatype,root,comm,ierr)
</b></tt></nobr></td></tr><tr></tr></tbody></table>
</p></ul><p><br>
 
<b><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Scatter.txt" target="_blank"> 
MPI_Scatter</a></b>
</p><p>
</p><ul>Data movement operation.
    Distributes distinct messages from a single source task to each task in
    the group.  
    <br><font size="-1"><b><input type="button" value="Diagram Here" onclick="popUp(&#39;images/MPI_Scatter.gif&#39;)"> </b></font>
    <!--                                                  
    <A HREF="images/MPI_Scatter.gif" TARGET=_blank> Diagram here.</A>  
    -->
<p>
<table border="1" cellpadding="5" cellspacing="0" width="75%">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Scatter (&amp;sendbuf,sendcnt,sendtype,&amp;recvbuf,   <br>
        <font color="#FFFFFF">......</font> 
                recvcnt,recvtype,root,comm)  <br>
    MPI_SCATTER (sendbuf,sendcnt,sendtype,recvbuf,  <br> 
        <font color="#FFFFFF">......</font> 
                recvcnt,recvtype,root,comm,ierr)
</b></tt></nobr></td></tr><tr></tr></tbody></table>
</p></ul><p><br>

<b><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Gather.txt" target="_blank"> 
MPI_Gather</a></b>
</p><p>
</p><ul>Data movement operation.
    Gathers distinct messages from each task in the group to a single
    destination task.  This routine is the reverse operation of MPI_Scatter.
    <br><font size="-1"><b><input type="button" value="Diagram Here" onclick="popUp(&#39;images/MPI_Gather.gif&#39;)"> </b></font>
    <!--                                                  
    <A HREF="images/MPI_Gather.gif" TARGET=_blank> Diagram here.</A>
    -->
<p>
<table border="1" cellpadding="5" cellspacing="0" width="75%">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Gather (&amp;sendbuf,sendcnt,sendtype,&amp;recvbuf,  <br>
        <font color="#FFFFFF">......</font> 
               recvcount,recvtype,root,comm)  <br>
    MPI_GATHER (sendbuf,sendcnt,sendtype,recvbuf,  <br>
        <font color="#FFFFFF">......</font> 
               recvcount,recvtype,root,comm,ierr)  
</b></tt></nobr></td></tr><tr></tr></tbody></table>
</p></ul><p><br>

<b><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Allgather.txt" target="_blank"> 
MPI_Allgather</a></b>
</p><p>
</p><ul>Data movement operation.
    Concatenation of data to all tasks in a group.  Each task in the group,
    in effect, performs a one-to-all broadcasting operation within the
    group.
    <br><font size="-1"><b><input type="button" value="Diagram Here" onclick="popUp(&#39;images/MPI_Allgather.gif&#39;)"> </b></font>
    <!--                                                  
    <A HREF="images/MPI_Allgather.gif" TARGET=_blank> Diagram here.</A>
    -->
<p>
<table border="1" cellpadding="5" cellspacing="0" width="75%">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Allgather (&amp;sendbuf,sendcount,sendtype,&amp;recvbuf,  <br>
        <font color="#FFFFFF">......</font> 
                  recvcount,recvtype,comm) <br>
    MPI_ALLGATHER (sendbuf,sendcount,sendtype,recvbuf, <br> 
        <font color="#FFFFFF">......</font> 
                  recvcount,recvtype,comm,info)
</b></tt></nobr></td></tr><tr></tr></tbody></table>
</p></ul><p><br>

<b><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Reduce.txt" target="_blank"> 
MPI_Reduce</a></b>
</p><p>
</p><ul>Collective computation operation.
    Applies a reduction operation on all tasks in the group and places the
    result in one task.  
    <br><font size="-1"><b><input type="button" value="Diagram Here" onclick="popUp(&#39;images/MPI_Reduce.gif&#39;)"> </b></font>
    <!--                                                  
    <A HREF="images/MPI_Reduce.gif" TARGET=_blank> Diagram here.</A>
    -->
<p>
<table border="1" cellpadding="5" cellspacing="0" width="75%">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Reduce (&amp;sendbuf,&amp;recvbuf,count,datatype,op,root,comm) <br>
    MPI_REDUCE (sendbuf,recvbuf,count,datatype,op,root,comm,ierr)
</b></tt></nobr></td></tr><tr></tr></tbody></table>
</p><p><br>

    The predefined MPI reduction operations appear below.  Users can also 
    define their own reduction functions by using the
    <a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Op_create.txt" target="_blank">
    MPI_Op_create</a> routine.
</p><p>
<table border="1" cellspacing="0" cellpadding="5" width="90%">
<tbody><tr valign="TOP">
<th colspan="2">MPI Reduction Operation</th> 
<th>C Data Types</th>
<th>Fortran Data Type</th>
</tr><tr valign="TOP">
<td bgcolor="#FOF5FE"><tt><b> MPI_MAX    
</b></tt></td><td>maximum       
</td><td>integer, float      
</td><td>integer, real, complex  
</td></tr><tr valign="TOP">
<td bgcolor="#FOF5FE"><tt><b>MPI_MIN    
</b></tt></td><td>minimum       
</td><td>integer, float      
</td><td>integer, real, complex  
</td></tr><tr valign="TOP">
<td bgcolor="#FOF5FE"><tt><b>MPI_SUM    
</b></tt></td><td>sum          
</td><td>integer, float      
</td><td>integer, real, complex  
</td></tr><tr valign="TOP">
<td bgcolor="#FOF5FE"><tt><b>MPI_PROD    
</b></tt></td><td>product      
</td><td>integer, float      
</td><td>integer, real, complex  
</td></tr><tr valign="TOP">
<td bgcolor="#FOF5FE"><tt><b>MPI_LAND    
</b></tt></td><td>logical AND   
</td><td>integer           
</td><td>logical                 
</td></tr><tr valign="TOP">
<td bgcolor="#FOF5FE"><tt><b>MPI_BAND    
</b></tt></td><td>bit-wise AND  
</td><td>integer, MPI_BYTE   
</td><td>integer, MPI_BYTE      
</td></tr><tr valign="TOP">
<td bgcolor="#FOF5FE"><tt><b>MPI_LOR    
</b></tt></td><td>logical OR    
</td><td>integer            
</td><td>logical                 
</td></tr><tr valign="TOP">
<td bgcolor="#FOF5FE"><tt><b>MPI_BOR    
</b></tt></td><td>bit-wise OR   
</td><td>integer, MPI_BYTE   
</td><td>integer, MPI_BYTE       
</td></tr><tr valign="TOP">
<td bgcolor="#FOF5FE"><tt><b>MPI_LXOR    
</b></tt></td><td>logical XOR   
</td><td>integer           
</td><td>logical                 
</td></tr><tr valign="TOP">
<td bgcolor="#FOF5FE"><tt><b>MPI_BXOR    
</b></tt></td><td>bit-wise XOR  
</td><td>integer, MPI_BYTE   
</td><td>integer, MPI_BYTE      
</td></tr><tr valign="TOP">
<td bgcolor="#FOF5FE"><tt><b>MPI_MAXLOC  
</b></tt></td><td>max value and location
</td><td>float, double and long double         
</td><td>real, complex,double precision        
</td></tr><tr valign="TOP">
<td bgcolor="#FOF5FE"><tt><b>MPI_MINLOC  
</b></tt></td><td>min value and location 
</td><td>float, double and long double         
</td><td>real, complex, double precision        
</td></tr></tbody></table>
</p><p>
</p><li>Note from the MPI_Reduce man page:
The operation is always assumed to be associative. All predefined operations are also assumed to be commutative. Users may define operations that are assumed to be associative, but not commutative. The "canonical" evaluation order of a reduction is determined by the ranks of the processes in the group. However, the implementation can take advantage of associativity, or associativity and commutativity in order to change the order of evaluation. This may change the result of the reduction for operations that are not strictly associative and commutative, such as floating point addition. [Advice to implementors]
It is strongly recommended that MPI_REDUCE be implemented so that the same result be obtained whenever the function is applied on the same arguments, appearing in the same order. Note that this may prevent optimizations that take advantage of the physical location of processors. [End of advice to implementors]
</li></ul>
<p> 

<b><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Allreduce.txt" target="_blank"> 
MPI_Allreduce</a></b>
</p><p>
</p><ul>Collective computation operation + data movement.
    Applies a reduction operation and places the result in all tasks in the
    group.  This is equivalent to an MPI_Reduce followed by an MPI_Bcast.
    <br><font size="-1"><b><input type="button" value="Diagram Here" onclick="popUp(&#39;images/MPI_Allreduce.gif&#39;)"> </b></font>
    <!--                                                  
    <A HREF="images/MPI_Allreduce.gif" TARGET=_blank> Diagram here.</A>
    -->
<p>
<table border="1" cellpadding="5" cellspacing="0" width="75%">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Allreduce (&amp;sendbuf,&amp;recvbuf,count,datatype,op,comm) <br>
    MPI_ALLREDUCE (sendbuf,recvbuf,count,datatype,op,comm,ierr)
</b></tt></nobr></td></tr><tr></tr></tbody></table>
</p></ul><p><br>

<b><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Reduce_scatter.txt" target="_blank"> 
MPI_Reduce_scatter</a></b>
</p><p>
</p><ul>Collective computation operation + data movement.
    First does an element-wise reduction on a vector across all tasks in the
    group.  Next, the result vector is split into disjoint segments and
    distributed across the tasks.  This is equivalent to an MPI_Reduce 
    followed by an MPI_Scatter operation.
    <br><font size="-1"><b><input type="button" value="Diagram Here" onclick="popUp(&#39;images/MPI_Reduce_scatter.gif&#39;)"> </b></font>
    <!--                                                  
    <A HREF="images/MPI_Reduce_scatter.gif" TARGET=_blank> Diagram here.</A>
    -->
<p>
<table border="1" cellpadding="5" cellspacing="0" width="75%">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Reduce_scatter (&amp;sendbuf,&amp;recvbuf,recvcount,datatype, <br>
        <font color="#FFFFFF">......</font>
         op,comm) <br>
    MPI_REDUCE_SCATTER (sendbuf,recvbuf,recvcount,datatype, <br>
        <font color="#FFFFFF">......</font>
         op,comm,ierr)
</b></tt></nobr></td></tr><tr></tr></tbody></table>
</p></ul><p><br>

<b><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Alltoall.txt" target="_blank"> 
MPI_Alltoall</a></b>
</p><p>
</p><ul>Data movement operation.
    Each task in a group performs a scatter operation, sending a distinct
    message to all the tasks in the group in order by index.
    <br><font size="-1"><b><input type="button" value="Diagram Here" onclick="popUp(&#39;images/MPI_Alltoall.gif&#39;)"> </b></font>
    <!--                                                  
    <A HREF="images/MPI_Alltoall.gif" TARGET=_blank>Diagram here.</A>
    -->
<p>
<table border="1" cellpadding="5" cellspacing="0" width="75%">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Alltoall (&amp;sendbuf,sendcount,sendtype,&amp;recvbuf, <br>
        <font color="#FFFFFF">......</font>
                 recvcnt,recvtype,comm) <br>
    MPI_ALLTOALL (sendbuf,sendcount,sendtype,recvbuf, <br>
        <font color="#FFFFFF">......</font>
                 recvcnt,recvtype,comm,ierr)
</b></tt></nobr></td></tr><tr></tr></tbody></table>
</p></ul><p><br>

<b><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Scan.txt" target="_blank"> 
MPI_Scan</a></b>
</p><p>
</p><ul>Performs a scan operation with respect to a reduction operation across
    a task group.
    <br><font size="-1"><b><input type="button" value="Diagram Here" onclick="popUp(&#39;images/MPI_Scan.gif&#39;)"> </b></font>
    <!--                                                  
    <A HREF="images/MPI_Scan.gif"TARGET=_blank>Diagram here.</A>
    -->
<p>
<table border="1" cellpadding="5" cellspacing="0" width="75%">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Scan (&amp;sendbuf,&amp;recvbuf,count,datatype,op,comm) <br>
    MPI_SCAN (sendbuf,recvbuf,count,datatype,op,comm,ierr)
</b></tt></nobr></td></tr><tr></tr></tbody></table>
</p></ul><p><br>

</p><p></p><hr><p>

</p><h2>Examples: Collective Communications</h2>

<ul>
<p>
Perform a scatter operation on the rows of an array
</p><p>
<table border="1" cellpadding="15" cellspacing="0" width="90%"><tbody><tr><td> <!---outer table--->
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tbody><tr>
<td width="30" bgcolor="FOF5FE" colspan="2" align="center"><img src="./Message Passing Interface (MPI)_files/page01.gif"></td>
<td bgcolor="FOF5FE"><b><br>&nbsp;&nbsp;&nbsp;&nbsp;
C Language - Collective Communications Example
</b><p></p></td>

</tr><tr valign="top"><td colspan="3"><p></p></td>

</tr><tr valign="top">
<td width="30"><pre><font color="#AAAAAA"> 1<br> 2<br> 3<br> 4<br> 5<br> 6<br> 7<br> 8<br> 9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33</font></pre></td>

<td width="1" bgcolor="#7099cc"></td>

<td><pre>   #include <font color="#DF4442">"mpi.h"</font>
   #include &lt;stdio.h&gt;
   #define SIZE 4

   main(int argc, char *argv[])  {
   int numtasks, rank, sendcount, recvcount, source;
   float sendbuf[SIZE][SIZE] = {
     {1.0, 2.0, 3.0, 4.0},
     {5.0, 6.0, 7.0, 8.0},
     {9.0, 10.0, 11.0, 12.0},
     {13.0, 14.0, 15.0, 16.0}  };
   float recvbuf[SIZE];

   <font color="#DF4442">MPI_Init</font>(&amp;argc,&amp;argv);
   <font color="#DF4442">MPI_Comm_rank</font>(MPI_COMM_WORLD, &amp;rank);
   <font color="#DF4442">MPI_Comm_size</font>(MPI_COMM_WORLD, &amp;numtasks);

   if (numtasks == SIZE) {
     <font color="#AAAAAA">// define source task and elements to send/receive, then perform collective scatter</font>
     source = 1;
     sendcount = SIZE;
     recvcount = SIZE;
     <font color="#DF4442">MPI_Scatter</font>(sendbuf,sendcount,MPI_FLOAT,recvbuf,recvcount,
                 MPI_FLOAT,source,MPI_COMM_WORLD);

     printf("rank= %d  Results: %f %f %f %f\n",rank,recvbuf[0],
            recvbuf[1],recvbuf[2],recvbuf[3]);
     }
   else
     printf("Must specify %d processors. Terminating.\n",SIZE);

   <font color="#DF4442">MPI_Finalize</font>();
   }

</pre></td>
</tr></tbody></table>
</td></tr></tbody></table>  <!---outer table--->

<br><br><br>

<table border="1" cellpadding="15" cellspacing="0" width="90%"><tbody><tr><td> <!---outer table--->
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tbody><tr>
<td width="30" bgcolor="FOF5FE" colspan="2" align="center"><img src="./Message Passing Interface (MPI)_files/page01.gif"></td>
<td bgcolor="FOF5FE"><b><br>&nbsp;&nbsp;&nbsp;&nbsp;
Fortran - Collective Communications Example
</b><p></p></td>

</tr><tr valign="top"><td colspan="3"><p></p></td>

</tr><tr valign="top">
<td width="30"><pre><font color="#AAAAAA"> 1<br> 2<br> 3<br> 4<br> 5<br> 6<br> 7<br> 8<br> 9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36</font></pre></td>

<td width="1" bgcolor="#7099cc"></td>

<td><pre>   program scatter
   include <font color="#DF4442">'mpif.h'</font>

   integer SIZE
   parameter(SIZE=4)
   integer numtasks, rank, sendcount, recvcount, source, ierr
   real*4 sendbuf(SIZE,SIZE), recvbuf(SIZE)

   <font color="#AAAAAA">! Fortran stores this array in column major order, so the 
   ! scatter will actually scatter columns, not rows.</font>
   data sendbuf /1.0, 2.0, 3.0, 4.0, &amp;
                 5.0, 6.0, 7.0, 8.0, &amp;
                 9.0, 10.0, 11.0, 12.0, &amp;
                 13.0, 14.0, 15.0, 16.0 /

   call <font color="#DF4442">MPI_INIT</font>(ierr)
   call <font color="#DF4442">MPI_COMM_RANK</font>(MPI_COMM_WORLD, rank, ierr)
   call <font color="#DF4442">MPI_COMM_SIZE</font>(MPI_COMM_WORLD, numtasks, ierr)

   if (numtasks .eq. SIZE) then
      <font color="#AAAAAA">! define source task and elements to send/receive, then perform collective scatter</font>
      source = 1
      sendcount = SIZE
      recvcount = SIZE
      call <font color="#DF4442">MPI_SCATTER</font>(sendbuf, sendcount, MPI_REAL, recvbuf, recvcount, MPI_REAL, &amp;
                       source, MPI_COMM_WORLD, ierr)

      print *, 'rank= ',rank,' Results: ',recvbuf 

   else
      print *, 'Must specify',SIZE,' processors.  Terminating.' 
   endif

   call <font color="#DF4442">MPI_FINALIZE</font>(ierr)

   end

</pre></td>
</tr></tbody></table>
</td></tr></tbody></table>  <!---outer table--->

<br><br>
Sample program output:
</p><pre>rank= 0  Results: 1.000000 2.000000 3.000000 4.000000
rank= 1  Results: 5.000000 6.000000 7.000000 8.000000
rank= 2  Results: 9.000000 10.000000 11.000000 12.000000
rank= 3  Results: 13.000000 14.000000 15.000000 16.000000
</pre>
</ul>

<!--------------------------------------------------------------------------->

<a name="Derived_Data_Types"> <br><br>  </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Derived Data Types</span></td>
</tr></tbody></table>
<br>

<ul>
<p>
</p><li>As <a href="https://computing.llnl.gov/tutorials/mpi/#Routine_Arguments">previously mentioned</a>, MPI 
    predefines its primitive data types:
<p>
<table border="1" cellspacing="0" cellpadding="5" width="90%">
<tbody><tr><th colspan="2">C Data Types</th>
<th>Fortran Data Types</th>
</tr><tr valign="top">
<td width="33%"><pre>MPI_CHAR
MPI_WCHAR
MPI_SHORT
MPI_INT
MPI_LONG
MPI_LONG_LONG_INT 
MPI_LONG_LONG	 	 
MPI_SIGNED_CHAR
MPI_UNSIGNED_CHAR
MPI_UNSIGNED_SHORT
MPI_UNSIGNED_LONG
MPI_UNSIGNED
MPI_FLOAT
MPI_DOUBLE
MPI_LONG_DOUBLE
</pre></td>
<td width="33%"><pre>MPI_C_COMPLEX
MPI_C_FLOAT_COMPLEX
MPI_C_DOUBLE_COMPLEX
MPI_C_LONG_DOUBLE_COMPLEX	 	 
MPI_C_BOOL
MPI_LOGICAL
MPI_C_LONG_DOUBLE_COMPLEX 	 
MPI_INT8_T 
MPI_INT16_T
MPI_INT32_T 
MPI_INT64_T	 	 
MPI_UINT8_T 
MPI_UINT16_T 
MPI_UINT32_T 
MPI_UINT64_T
MPI_BYTE
MPI_PACKED
</pre></td>
<td width="33%"><pre>MPI_CHARACTER
MPI_INTEGER
MPI_INTEGER1 
MPI_INTEGER2
MPI_INTEGER4
MPI_REAL
MPI_REAL2 
MPI_REAL4
MPI_REAL8
MPI_DOUBLE_PRECISION
MPI_COMPLEX
MPI_DOUBLE_COMPLEX
MPI_LOGICAL
MPI_BYTE
MPI_PACKED
</pre></td>
</tr></tbody></table>
</p><p>
</p></li><li>MPI also provides facilities for you to define your own data structures 
    based upon sequences of the MPI primitive data types. Such user defined 
    structures are called derived data types. 
<p>
</p></li><li>Primitive data types are contiguous. Derived data types allow you to 
    specify non-contiguous data in a convenient manner and to treat it as 
    though it was contiguous.
<p>
</p></li><li>MPI provides several methods for constructing derived data types:
    <ul>
    <li>Contiguous
    </li><li>Vector
    </li><li>Indexed
    </li><li>Struct
    </li></ul>
</li></ul>

<p></p><hr><p>

</p><h2>Derived Data Type Routines</h2>
<p>

<b><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Type_contiguous.txt" target="_blank"> 
MPI_Type_contiguous</a></b>
</p><p>
</p><ul>The simplest constructor. Produces a new data type by making count 
    copies of an existing data type. 
<p>
<table border="1" cellpadding="5" cellspacing="0" width="75%">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Type_contiguous (count,oldtype,&amp;newtype) <br>
    MPI_TYPE_CONTIGUOUS (count,oldtype,newtype,ierr)
</b></tt></nobr></td></tr><tr></tr></tbody></table>
</p></ul><p><br>

<b><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Type_vector.txt" target="_blank"> 
MPI_Type_vector</a> 
<br><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Type_hvector.txt" target="_blank"> 
MPI_Type_hvector</a></b>
</p><p>
</p><ul>Similar to contiguous, but allows for regular gaps (stride) in the 
    displacements.  MPI_Type_hvector is identical to
    MPI_Type_vector except that stride is specified in bytes.
<p>
<table border="1" cellpadding="5" cellspacing="0" width="75%">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Type_vector (count,blocklength,stride,oldtype,&amp;newtype)<br> 
    MPI_TYPE_VECTOR (count,blocklength,stride,oldtype,newtype,ierr)
</b></tt></nobr></td></tr><tr></tr></tbody></table>
</p></ul><p><br>
    
<b><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Type_indexed.txt" target="_blank"> 
MPI_Type_indexed</a> 
<br><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Type_hindexed.txt" target="_blank"> 
MPI_Type_hindexed</a></b>
</p><p>
</p><ul>An array of displacements of the input data type is provided as the map 
    for the new data type.  MPI_Type_hindexed is identical to 
    MPI_Type_indexed except that offsets are specified in bytes.
<p>
<table border="1" cellpadding="5" cellspacing="0" width="75%">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Type_indexed (count,blocklens[],offsets[],old_type,&amp;newtype)<br>
    MPI_TYPE_INDEXED (count,blocklens(),offsets(),old_type,newtype,ierr)
</b></tt></nobr></td></tr><tr></tr></tbody></table>
</p></ul><p><br>
    
<b><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Type_struct.txt" target="_blank"> 
MPI_Type_struct</a></b>
</p><p>
</p><ul>The new data type is formed according to completely defined map of the 
    component data types.
<br><b>NOTE:</b> This function is deprecated in MPI-2.0 and replaced by
    MPI_Type_create_struct in MPI-3.0
<p>
<table border="1" cellpadding="5" cellspacing="0" width="75%">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Type_struct (count,blocklens[],offsets[],old_types,&amp;newtype)<br>
    MPI_TYPE_STRUCT (count,blocklens(),offsets(),old_types,newtype,ierr)
</b></tt></nobr></td></tr><tr></tr></tbody></table>
</p></ul><p><br>

<b><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Type_extent.txt" target="_blank"> 
MPI_Type_extent</a></b>
</p><p>
</p><ul>Returns the size in bytes of the specified data type. Useful for
    the MPI subroutines that require specification of offsets in bytes.
<br><b>NOTE:</b> This function is deprecated in MPI-2.0 and replaced by
    MPI_Type_get_extent in MPI-3.0
<p>
<table border="1" cellpadding="5" cellspacing="0" width="75%">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Type_extent (datatype,&amp;extent)<br>
    MPI_TYPE_EXTENT (datatype,extent,ierr)
</b></tt></nobr></td></tr><tr></tr></tbody></table>
</p></ul><p><br>

<b><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Type_commit.txt" target="_blank"> 
MPI_Type_commit</a></b>
</p><p>
</p><ul>Commits new datatype to the system. Required for all user constructed
    (derived) datatypes.
<p>
<table border="1" cellpadding="5" cellspacing="0" width="75%">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Type_commit (&amp;datatype)<br>
    MPI_TYPE_COMMIT (datatype,ierr)
</b></tt></nobr></td></tr><tr></tr></tbody></table>
</p></ul><p><br>

<b><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Type_free.txt" target="_blank">
MPI_Type_free</a></b>
</p><p>
</p><ul>Deallocates the specified datatype object. Use of this routine is
    especially important to prevent memory exhaustion if many datatype 
    objects are created, as in a loop.
<p>
<table border="1" cellpadding="5" cellspacing="0" width="75%">
<tbody><tr valign="top"><td><nobr><tt><b> 
    MPI_Type_free (&amp;datatype)<br>
    MPI_TYPE_FREE (datatype,ierr)
</b></tt></nobr></td></tr><tr></tr></tbody></table>
</p></ul><p><br>

</p><p></p><hr><p>

</p><h2>Examples: Contiguous Derived Data Type</h2>

<ul>
<p>
Create a data type representing a row of an array and distribute a
different row to all processes.
<br><font size="-1"><b><input type="button" value="Diagram Here" onclick="popUp(&#39;images/MPI_Type_contiguous.gif&#39;)"> </b></font>
<!--
<A HREF=images/MPI_Type_contiguous.gif TARGET=_blank>Diagram 
here.</A>
-->
</p><p><br>

<table border="1" cellpadding="15" cellspacing="0" width="90%"><tbody><tr><td> <!---outer table--->
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tbody><tr>
<td width="30" bgcolor="FOF5FE" colspan="2" align="center"><img src="./Message Passing Interface (MPI)_files/page01.gif"></td>
<td bgcolor="FOF5FE"><b><br>&nbsp;&nbsp;&nbsp;&nbsp;
C Language - Contiguous Derived Data Type Example
</b><p></p></td>

</tr><tr valign="top"><td colspan="3"><p></p></td>

</tr><tr valign="top">
<td width="30"><pre><font color="#AAAAAA"> 1<br> 2<br> 3<br> 4<br> 5<br> 6<br> 7<br> 8<br> 9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43</font></pre></td>

<td width="1" bgcolor="#7099cc"></td>

<td><pre>   #include <font color="#DF4442">"mpi.h"</font>
   #include &lt;stdio.h&gt;
   #define SIZE 4

   main(int argc, char *argv[])  {
   int numtasks, rank, source=0, dest, tag=1, i;
   float a[SIZE][SIZE] =
     {1.0, 2.0, 3.0, 4.0,
      5.0, 6.0, 7.0, 8.0,
      9.0, 10.0, 11.0, 12.0,
      13.0, 14.0, 15.0, 16.0};
   float b[SIZE];

   <font color="#DF4442">MPI_Status stat</font>;
   <font color="#DF4442">MPI_Datatype rowtype</font>;   <font color="#AAAAAA">// required variable</font>

   <font color="#DF4442">MPI_Init</font>(&amp;argc,&amp;argv);
   <font color="#DF4442">MPI_Comm_rank</font>(MPI_COMM_WORLD, &amp;rank);
   <font color="#DF4442">MPI_Comm_size</font>(MPI_COMM_WORLD, &amp;numtasks);

   <font color="#AAAAAA">// create contiguous derived data type</font>
   <font color="#DF4442">MPI_Type_contiguous</font>(SIZE, MPI_FLOAT, &amp;rowtype);
   <font color="#DF4442">MPI_Type_commit</font>(&amp;rowtype);

   if (numtasks == SIZE) {
      <font color="#AAAAAA">// task 0 sends one element of rowtype to all tasks</font>
      if (rank == 0) {
         for (i=0; i&lt;numtasks; i++)
           <font color="#DF4442">MPI_Send</font>(&amp;a[i][0], 1, rowtype, i, tag, MPI_COMM_WORLD);
         }

      <font color="#AAAAAA">// all tasks receive rowtype data from task 0</font>
      <font color="#DF4442">MPI_Recv</font>(b, SIZE, MPI_FLOAT, source, tag, MPI_COMM_WORLD, &amp;stat);
      printf("rank= %d  b= %3.1f %3.1f %3.1f %3.1f\n",
             rank,b[0],b[1],b[2],b[3]);
      }
   else
      printf("Must specify %d processors. Terminating.\n",SIZE);

   <font color="#AAAAAA">// free datatype when done using it</font>
   <font color="#DF4442">MPI_Type_free</font>(&amp;rowtype);
   <font color="#DF4442">MPI_Finalize</font>();
   }

</pre></td>
</tr></tbody></table>
</td></tr></tbody></table>  <!---outer table--->

<br><br><br>

<table border="1" cellpadding="15" cellspacing="0" width="90%"><tbody><tr><td> <!---outer table--->
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tbody><tr>
<td width="30" bgcolor="FOF5FE" colspan="2" align="center"><img src="./Message Passing Interface (MPI)_files/page01.gif"></td>
<td bgcolor="FOF5FE"><b><br>&nbsp;&nbsp;&nbsp;&nbsp;
Fortran - Contiguous Derived Data Type Example
</b><p></p></td>

</tr><tr valign="top"><td colspan="3"><p></p></td>

</tr><tr valign="top">
<td width="30"><pre><font color="#AAAAAA"> 1<br> 2<br> 3<br> 4<br> 5<br> 6<br> 7<br> 8<br> 9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46</font></pre></td>

<td width="1" bgcolor="#7099cc"></td>

<td><pre>   program contiguous
   include <font color="#DF4442">'mpif.h'</font>

   integer SIZE
   parameter(SIZE=4)
   integer numtasks, rank, source, dest, tag, i,  ierr
   real*4 a(0:SIZE-1,0:SIZE-1), b(0:SIZE-1)
   integer <font color="#DF4442">stat(MPI_STATUS_SIZE)</font>
   integer <font color="#DF4442">columntype</font>   <font color="#AAAAAA">! required variable</font>
   tag = 1

   <font color="#AAAAAA">! Fortran stores this array in column major order</font>
   data a  /1.0, 2.0, 3.0, 4.0, &amp;
            5.0, 6.0, 7.0, 8.0, &amp;
            9.0, 10.0, 11.0, 12.0, &amp; 
            13.0, 14.0, 15.0, 16.0 /

   call <font color="#DF4442">MPI_INIT</font>(ierr)
   call <font color="#DF4442">MPI_COMM_RANK</font>(MPI_COMM_WORLD, rank, ierr)
   call <font color="#DF4442">MPI_COMM_SIZE</font>(MPI_COMM_WORLD, numtasks, ierr)

   <font color="#AAAAAA">! create contiguous derived data type</font>
   call <font color="#DF4442">MPI_TYPE_CONTIGUOUS</font>(SIZE, MPI_REAL, columntype, ierr)
   call <font color="#DF4442">MPI_TYPE_COMMIT</font>(columntype, ierr)
  
   if (numtasks .eq. SIZE) then
      <font color="#AAAAAA">! task 0 sends one element of columntype to all tasks</font>
      if (rank .eq. 0) then
         do i=0, numtasks-1
         call <font color="#DF4442">MPI_SEND</font>(a(0,i), 1, columntype, i, tag, MPI_COMM_WORLD,ierr)
         end do
      endif

      <font color="#AAAAAA">! all tasks receive columntype data from task 0</font>
      source = 0
      call <font color="#DF4442">MPI_RECV</font>(b, SIZE, MPI_REAL, source, tag, MPI_COMM_WORLD, stat, ierr)
      print *, 'rank= ',rank,' b= ',b
   else
      print *, 'Must specify',SIZE,' processors.  Terminating.' 
   endif

   <font color="#AAAAAA">! free datatype when done using it</font>
   call <font color="#DF4442">MPI_TYPE_FREE</font>(columntype, ierr)
   call <font color="#DF4442">MPI_FINALIZE</font>(ierr)

   end

</pre></td>
</tr></tbody></table>
</td></tr></tbody></table>  <!---outer table--->


<br><br>
Sample program output:
</p><pre>rank= 0  b= 1.0 2.0 3.0 4.0
rank= 1  b= 5.0 6.0 7.0 8.0
rank= 2  b= 9.0 10.0 11.0 12.0
rank= 3  b= 13.0 14.0 15.0 16.0
</pre>
</ul>

<p><br></p><hr><p><br>

</p><h2>Examples: Vector Derived Data Type</h2>

<ul>
<p>
Create a data type representing a column of an array and distribute
different columns to all processes.
<br><font size="-1"><b><input type="button" value="Diagram Here" onclick="popUp(&#39;images/MPI_Type_vector.gif&#39;)"> </b></font>
<!--
<A HREF=images/MPI_Type_vector.gif TARGET=_blank>Diagram here.</A>
-->
</p><p><br>

<table border="1" cellpadding="15" cellspacing="0" width="90%"><tbody><tr><td> <!---outer table--->
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tbody><tr>
<td width="30" bgcolor="FOF5FE" colspan="2" align="center"><img src="./Message Passing Interface (MPI)_files/page01.gif"></td>
<td bgcolor="FOF5FE"><b><br>&nbsp;&nbsp;&nbsp;&nbsp;
C Language - Vector Derived Data Type Example
</b><p></p></td>

</tr><tr valign="top"><td colspan="3"><p></p></td>

</tr><tr valign="top">
<td width="30"><pre><font color="#AAAAAA"> 1<br> 2<br> 3<br> 4<br> 5<br> 6<br> 7<br> 8<br> 9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44</font></pre></td>

<td width="1" bgcolor="#7099cc"></td>

<td><pre>   #include <font color="#DF4442">"mpi.h"</font>
   #include &lt;stdio.h&gt;
   #define SIZE 4

   main(int argc, char *argv[])  {
   int numtasks, rank, source=0, dest, tag=1, i;
   float a[SIZE][SIZE] = 
     {1.0, 2.0, 3.0, 4.0,  
      5.0, 6.0, 7.0, 8.0, 
      9.0, 10.0, 11.0, 12.0,
     13.0, 14.0, 15.0, 16.0};
   float b[SIZE]; 

   <font color="#DF4442">MPI_Status stat</font>;
   <font color="#DF4442">MPI_Datatype columntype</font>;   <font color="#AAAAAA">// required variable</font>


   <font color="#DF4442">MPI_Init</font>(&amp;argc,&amp;argv);
   <font color="#DF4442">MPI_Comm_rank</font>(MPI_COMM_WORLD, &amp;rank);
   <font color="#DF4442">MPI_Comm_size</font>(MPI_COMM_WORLD, &amp;numtasks);
   
   <font color="#AAAAAA">// create vector derived data type</font>
   <font color="#DF4442">MPI_Type_vector</font>(SIZE, 1, SIZE, MPI_FLOAT, &amp;columntype);
   <font color="#DF4442">MPI_Type_commit</font>(&amp;columntype);

   if (numtasks == SIZE) {
      <font color="#AAAAAA">// task 0 sends one element of columntype to all tasks</font>
      if (rank == 0) {
         for (i=0; i&lt;numtasks; i++) 
            <font color="#DF4442">MPI_Send</font>(&amp;a[0][i], 1, columntype, i, tag, MPI_COMM_WORLD);
         }
 
      <font color="#AAAAAA">// all tasks receive columntype data from task 0</font>
      <font color="#DF4442">MPI_Recv</font>(b, SIZE, MPI_FLOAT, source, tag, MPI_COMM_WORLD, &amp;stat);
      printf("rank= %d  b= %3.1f %3.1f %3.1f %3.1f\n",
             rank,b[0],b[1],b[2],b[3]);
      }
   else
      printf("Must specify %d processors. Terminating.\n",SIZE);

   <font color="#AAAAAA">// free datatype when done using it</font>
   <font color="#DF4442">MPI_Type_free</font>(&amp;columntype);
   <font color="#DF4442">MPI_Finalize</font>();
   }

</pre></td>
</tr></tbody></table>
</td></tr></tbody></table>  <!---outer table--->

<br><br><br>

<table border="1" cellpadding="15" cellspacing="0" width="90%"><tbody><tr><td> <!---outer table--->
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tbody><tr>
<td width="30" bgcolor="FOF5FE" colspan="2" align="center"><img src="./Message Passing Interface (MPI)_files/page01.gif"></td>
<td bgcolor="FOF5FE"><b><br>&nbsp;&nbsp;&nbsp;&nbsp;
Fortran - Vector Derived Data Type Example
</b><p></p></td>

</tr><tr valign="top"><td colspan="3"><p></p></td>

</tr><tr valign="top">
<td width="30"><pre><font color="#AAAAAA"> 1<br> 2<br> 3<br> 4<br> 5<br> 6<br> 7<br> 8<br> 9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46</font></pre></td>

<td width="1" bgcolor="#7099cc"></td>

<td><pre>   program vector
   include <font color="#DF4442">'mpif.h'</font>

   integer SIZE
   parameter(SIZE=4)
   integer numtasks, rank, source, dest, tag, i,  ierr
   real*4 a(0:SIZE-1,0:SIZE-1), b(0:SIZE-1)
   integer <font color="#DF4442">stat(MPI_STATUS_SIZE)</font>
   integer <font color="#DF4442">rowtype</font>   <font color="#AAAAAA">! required variable</font>
   tag = 1

   <font color="#AAAAAA">! Fortran stores this array in column major order</font>
   data a  /1.0, 2.0, 3.0, 4.0, &amp;
            5.0, 6.0, 7.0, 8.0,  &amp;
            9.0, 10.0, 11.0, 12.0, &amp;
            13.0, 14.0, 15.0, 16.0 /

   call <font color="#DF4442">MPI_INIT</font>(ierr)
   call <font color="#DF4442">MPI_COMM_RANK</font>(MPI_COMM_WORLD, rank, ierr)
   call <font color="#DF4442">MPI_COMM_SIZE</font>(MPI_COMM_WORLD, numtasks, ierr)

   <font color="#AAAAAA">! create vector derived data type</font>
   call <font color="#DF4442">MPI_TYPE_VECTOR</font>(SIZE, 1, SIZE, MPI_REAL, rowtype, ierr)
   call <font color="#DF4442">MPI_TYPE_COMMIT</font>(rowtype, ierr)
  
   if (numtasks .eq. SIZE) then
      <font color="#AAAAAA">! task 0 sends one element of rowtype to all tasks</font>
      if (rank .eq. 0) then
         do i=0, numtasks-1
         call <font color="#DF4442">MPI_SEND</font>(a(i,0), 1, rowtype, i, tag, MPI_COMM_WORLD, ierr)
         end do
      endif

      <font color="#AAAAAA">! all tasks receive rowtype data from task 0</font>
      source = 0
      call <font color="#DF4442">MPI_RECV</font>(b, SIZE, MPI_REAL, source, tag, MPI_COMM_WORLD, stat, ierr)
      print *, 'rank= ',rank,' b= ',b
   else
      print *, 'Must specify',SIZE,' processors.  Terminating.' 
   endif

   <font color="#AAAAAA">! free datatype when done using it</font>
   call <font color="#DF4442">MPI_TYPE_FREE</font>(rowtype, ierr)
   call <font color="#DF4442">MPI_FINALIZE</font>(ierr)

   end

</pre></td>
</tr></tbody></table>
</td></tr></tbody></table>  <!---outer table--->

<br><br>

Sample program output:
</p><pre>rank= 0  b= 1.0 5.0 9.0 13.0
rank= 1  b= 2.0 6.0 10.0 14.0
rank= 2  b= 3.0 7.0 11.0 15.0
rank= 3  b= 4.0 8.0 12.0 16.0
</pre>
</ul>

<p><br></p><hr><p><br>

</p><h2>Examples: Indexed Derived Data Type</h2>

<ul>
<p>
Create a datatype by extracting variable portions of an array and distribute
to all tasks.  
<br><font size="-1"><b><input type="button" value="Diagram Here" onclick="popUp(&#39;images/MPI_Type_indexed.gif&#39;)"> </b></font>
<!--
<A HREF=images/MPI_Type_indexed.gif TARGET=_blank>Diagram here.</A>
-->
</p><p>
<table border="1" cellpadding="15" cellspacing="0" width="90%"><tbody><tr><td> <!---outer table--->
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tbody><tr>
<td width="30" bgcolor="FOF5FE" colspan="2" align="center"><img src="./Message Passing Interface (MPI)_files/page01.gif"></td>
<td bgcolor="FOF5FE"><b><br>&nbsp;&nbsp;&nbsp;&nbsp;
C Language - Indexed Derived Data Type Example
</b><p></p></td>

</tr><tr valign="top"><td colspan="3"><p></p></td>

</tr><tr valign="top">
<td width="30"><pre><font color="#AAAAAA"> 1<br> 2<br> 3<br> 4<br> 5<br> 6<br> 7<br> 8<br> 9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43</font></pre></td>

<td width="1" bgcolor="#7099cc"></td>

<td><pre>   #include <font color="#DF4442">"mpi.h"</font>
   #include &lt;stdio.h&gt;
   #define NELEMENTS 6

   main(int argc, char *argv[])  {
   int numtasks, rank, source=0, dest, tag=1, i;
   int blocklengths[2], displacements[2];
   float a[16] = 
     {1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 
      9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0};
   float b[NELEMENTS]; 

   <font color="#DF4442">MPI_Status stat</font>;
   <font color="#DF4442">MPI_Datatype indextype</font>;   <font color="#AAAAAA">// required variable</font>

   <font color="#DF4442">MPI_Init</font>(&amp;argc,&amp;argv);
   <font color="#DF4442">MPI_Comm_rank</font>(MPI_COMM_WORLD, &amp;rank);
   <font color="#DF4442">MPI_Comm_size</font>(MPI_COMM_WORLD, &amp;numtasks);

   blocklengths[0] = 4;
   blocklengths[1] = 2;
   displacements[0] = 5;
   displacements[1] = 12;
   
   <font color="#AAAAAA">// create indexed derived data type</font>
   <font color="#DF4442">MPI_Type_indexed</font>(2, blocklengths, displacements, MPI_FLOAT, &amp;indextype);
   <font color="#DF4442">MPI_Type_commit</font>(&amp;indextype);

   if (rank == 0) {
     for (i=0; i&lt;numtasks; i++) 
      <font color="#AAAAAA">// task 0 sends one element of indextype to all tasks</font>
        <font color="#DF4442">MPI_Send</font>(a, 1, indextype, i, tag, MPI_COMM_WORLD);
     }

   <font color="#AAAAAA">// all tasks receive indextype data from task 0</font>
   <font color="#DF4442">MPI_Recv</font>(b, NELEMENTS, MPI_FLOAT, source, tag, MPI_COMM_WORLD, &amp;stat);
   printf("rank= %d  b= %3.1f %3.1f %3.1f %3.1f %3.1f %3.1f\n",
          rank,b[0],b[1],b[2],b[3],b[4],b[5]);
   
   <font color="#AAAAAA">// free datatype when done using it</font>
   <font color="#DF4442">MPI_Type_free</font>(&amp;indextype);
   <font color="#DF4442">MPI_Finalize</font>();
   }

</pre></td>
</tr></tbody></table>
</td></tr></tbody></table>  <!---outer table--->

<br><br><br>

<table border="1" cellpadding="15" cellspacing="0" width="90%"><tbody><tr><td> <!---outer table--->
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tbody><tr>
<td width="30" bgcolor="FOF5FE" colspan="2" align="center"><img src="./Message Passing Interface (MPI)_files/page01.gif"></td>
<td bgcolor="FOF5FE"><b><br>&nbsp;&nbsp;&nbsp;&nbsp;
Fortran - Indexed Derived Data Type Example
</b><p></p></td>

</tr><tr valign="top"><td colspan="3"><p></p></td>

</tr><tr valign="top">
<td width="30"><pre><font color="#AAAAAA"> 1<br> 2<br> 3<br> 4<br> 5<br> 6<br> 7<br> 8<br> 9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47</font></pre></td>

<td width="1" bgcolor="#7099cc"></td>

<td><pre>   program indexed
   include <font color="#DF4442">'mpif.h'</font>

   integer NELEMENTS
   parameter(NELEMENTS=6)
   integer numtasks, rank, source, dest, tag, i,  ierr
   integer blocklengths(0:1), displacements(0:1)
   real*4 a(0:15), b(0:NELEMENTS-1)
   integer <font color="#DF4442">stat(MPI_STATUS_SIZE)</font>
   integer <font color="#DF4442">indextype</font>   <font color="#AAAAAA">! required variable</font>
   tag = 1

   data a  /1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, &amp;
            9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0 /

   call <font color="#DF4442">MPI_INIT</font>(ierr)
   call <font color="#DF4442">MPI_COMM_RANK</font>(MPI_COMM_WORLD, rank, ierr)
   call <font color="#DF4442">MPI_COMM_SIZE</font>(MPI_COMM_WORLD, numtasks, ierr)

   blocklengths(0) = 4
   blocklengths(1) = 2
   displacements(0) = 5
   displacements(1) = 12

   <font color="#AAAAAA">! create indexed derived data type</font>
   call <font color="#DF4442">MPI_TYPE_INDEXED</font>(2, blocklengths, displacements, MPI_REAL, &amp;
                         indextype, ierr)
   call <font color="#DF4442">MPI_TYPE_COMMIT</font>(indextype, ierr)
  
   if (rank .eq. 0) then
      <font color="#AAAAAA">! task 0 sends one element of indextype to all tasks</font>
      do i=0, numtasks-1
      call <font color="#DF4442">MPI_SEND</font>(a, 1, indextype, i, tag, MPI_COMM_WORLD, ierr)
      end do
   endif

   <font color="#AAAAAA">! all tasks receive indextype data from task 0</font>
   source = 0
   call <font color="#DF4442">MPI_RECV</font>(b, NELEMENTS, MPI_REAL, source, tag, MPI_COMM_WORLD, &amp;
                 stat, ierr)
   print *, 'rank= ',rank,' b= ',b

   <font color="#AAAAAA">! free datatype when done using it</font>
   call <font color="#DF4442">MPI_TYPE_FREE</font>(indextype, ierr)
   call <font color="#DF4442">MPI_FINALIZE</font>(ierr)

   end

</pre></td>
</tr></tbody></table>
</td></tr></tbody></table>  <!---outer table--->

<br><br>

Sample program output:
</p><pre>rank= 0  b= 6.0 7.0 8.0 9.0 13.0 14.0
rank= 1  b= 6.0 7.0 8.0 9.0 13.0 14.0
rank= 2  b= 6.0 7.0 8.0 9.0 13.0 14.0
rank= 3  b= 6.0 7.0 8.0 9.0 13.0 14.0
</pre>
</ul>

<p><br></p><hr><p><br>

</p><h2>Examples: Struct Derived Data Type</h2>

<ul>
<p>
Create a data type that represents a particle and distribute an array
of such particles to all processes.
<br><font size="-1"><b><input type="button" value="Diagram Here" onclick="popUp(&#39;images/MPI_Type_struct.gif&#39;)"> </b></font>
<!--
<A HREF=images/MPI_Type_struct.gif TARGET=_blank>Diagram here.</A>
-->
</p><p>


<table border="1" cellpadding="15" cellspacing="0" width="90%"><tbody><tr><td> <!---outer table--->
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tbody><tr>
<td width="30" bgcolor="FOF5FE" colspan="2" align="center"><img src="./Message Passing Interface (MPI)_files/page01.gif"></td>
<td bgcolor="FOF5FE"><b><br>&nbsp;&nbsp;&nbsp;&nbsp;
C Language - Struct Derived Data Type Example
</b><p></p></td>

</tr><tr valign="top"><td colspan="3"><p></p></td>

</tr><tr valign="top">
<td width="30"><pre><font color="#AAAAAA"> 1<br> 2<br> 3<br> 4<br> 5<br> 6<br> 7<br> 8<br> 9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58<br>59<br>60<br>61<br>62<br>63<br>64<br>65<br>66</font></pre></td>

<td width="1" bgcolor="#7099cc"></td>

<td><pre>   #include <font color="#DF4442">"mpi.h"</font>
   #include &lt;stdio.h&gt;
   #define NELEM 25

   main(int argc, char *argv[])  {
   int numtasks, rank, source=0, dest, tag=1, i;

   typedef struct {
     float x, y, z;
     float velocity;
     int  n, type;
     }          Particle;
   Particle     p[NELEM], particles[NELEM];
   <font color="#DF4442">MPI_Datatype particletype, oldtypes[2]</font>;   <font color="#AAAAAA">// required variables</font>
   int          blockcounts[2];

   <font color="#AAAAAA">// MPI_Aint type used to be consistent with syntax of</font>
   <font color="#AAAAAA">// MPI_Type_extent routine</font>
   <font color="#DF4442">MPI_Aint    offsets[2], extent</font>;

   <font color="#DF4442">MPI_Status stat</font>;

   <font color="#DF4442">MPI_Init</font>(&amp;argc,&amp;argv);
   <font color="#DF4442">MPI_Comm_rank</font>(MPI_COMM_WORLD, &amp;rank);
   <font color="#DF4442">MPI_Comm_size</font>(MPI_COMM_WORLD, &amp;numtasks);
 
   <font color="#AAAAAA">// setup description of the 4 MPI_FLOAT fields x, y, z, velocity</font>
   offsets[0] = 0;
   oldtypes[0] = MPI_FLOAT;
   blockcounts[0] = 4;

   <font color="#AAAAAA">// setup description of the 2 MPI_INT fields n, type</font>
   <font color="#AAAAAA">// need to first figure offset by getting size of MPI_FLOAT</font>
   <font color="#DF4442">MPI_Type_extent</font>(MPI_FLOAT, &amp;extent);
   offsets[1] = 4 * extent;
   oldtypes[1] = MPI_INT;
   blockcounts[1] = 2;

   <font color="#AAAAAA">// define structured type and commit it</font>
   <font color="#DF4442">MPI_Type_struct</font>(2, blockcounts, offsets, oldtypes, &amp;particletype);
   <font color="#DF4442">MPI_Type_commit</font>(&amp;particletype);

   <font color="#AAAAAA">// task 0 initializes the particle array and then sends it to each task</font>
   if (rank == 0) {
     for (i=0; i&lt;NELEM; i++) {
        particles[i].x = i * 1.0;
        particles[i].y = i * -1.0;
        particles[i].z = i * 1.0; 
        particles[i].velocity = 0.25;
        particles[i].n = i;
        particles[i].type = i % 2; 
        }
     for (i=0; i&lt;numtasks; i++) 
        <font color="#DF4442">MPI_Send</font>(particles, NELEM, particletype, i, tag, MPI_COMM_WORLD);
     }
 
   <font color="#AAAAAA">// all tasks receive particletype data</font>
   <font color="#DF4442">MPI_Recv</font>(p, NELEM, particletype, source, tag, MPI_COMM_WORLD, &amp;stat);

   printf("rank= %d   %3.2f %3.2f %3.2f %3.2f %d %d\n", rank,p[3].x,
        p[3].y,p[3].z,p[3].velocity,p[3].n,p[3].type);

   <font color="#AAAAAA">// free datatype when done using it</font>
   <font color="#DF4442">MPI_Type_free</font>(&amp;particletype);
   <font color="#DF4442">MPI_Finalize</font>();
   }

</pre></td>
</tr></tbody></table>
</td></tr></tbody></table>  <!---outer table--->

<br><br><br>

<table border="1" cellpadding="15" cellspacing="0" width="90%"><tbody><tr><td> <!---outer table--->
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tbody><tr>
<td width="30" bgcolor="FOF5FE" colspan="2" align="center"><img src="./Message Passing Interface (MPI)_files/page01.gif"></td>
<td bgcolor="FOF5FE"><b><br>&nbsp;&nbsp;&nbsp;&nbsp;
Fortan - Struct Derived Data Type Example
</b><p></p></td>

</tr><tr valign="top"><td colspan="3"><p></p></td>

</tr><tr valign="top">
<td width="30"><pre><font color="#AAAAAA"> 1<br> 2<br> 3<br> 4<br> 5<br> 6<br> 7<br> 8<br> 9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58<br>59<br>60<br>61<br>62<br>63</font></pre></td>

<td width="1" bgcolor="#7099cc"></td>

<td><pre>   program struct
   include <font color="#DF4442">'mpif.h'</font>

   integer NELEM
   parameter(NELEM=25)
   integer numtasks, rank, source, dest, tag, i,  ierr
   integer <font color="#DF4442">stat(MPI_STATUS_SIZE)</font>

   type Particle
   sequence
   real*4 x, y, z, velocity
   integer n, type
   end type Particle

   type (Particle) p(NELEM), particles(NELEM)
   integer <font color="#DF4442">particletype, oldtypes(0:1)</font>   <font color="#AAAAAA">! required variables</font>
   integer blockcounts(0:1), offsets(0:1), extent
   tag = 1

   call <font color="#DF4442">MPI_INIT</font>(ierr)
   call <font color="#DF4442">MPI_COMM_RANK</font>(MPI_COMM_WORLD, rank, ierr)
   call <font color="#DF4442">MPI_COMM_SIZE</font>(MPI_COMM_WORLD, numtasks, ierr)

   <font color="#AAAAAA">! setup description of the 4 MPI_REAL fields x, y, z, velocity</font>
   offsets(0) = 0
   oldtypes(0) = MPI_REAL
   blockcounts(0) = 4

   <font color="#AAAAAA">! setup description of the 2 MPI_INTEGER fields n, type</font> 
   <font color="#AAAAAA">! need to first figure offset by getting size of MPI_REAL</font>
   call <font color="#DF4442">MPI_TYPE_EXTENT</font>(MPI_REAL, extent, ierr)
   offsets(1) = 4 * extent
   oldtypes(1) = MPI_INTEGER
   blockcounts(1) = 2

   <font color="#AAAAAA">! define structured type and commit it</font> 
   call <font color="#DF4442">MPI_TYPE_STRUCT</font>(2, blockcounts, offsets, oldtypes, &amp;
                        particletype, ierr)
   call <font color="#DF4442">MPI_TYPE_COMMIT</font>(particletype, ierr)
  
   <font color="#AAAAAA">! task 0 initializes the particle array and then sends it to each task</font>
   if (rank .eq. 0) then
      do i=0, NELEM-1
      particles(i) = Particle ( 1.0*i, -1.0*i, 1.0*i, 0.25, i, mod(i,2) )
      end do

      do i=0, numtasks-1
      call <font color="#DF4442">MPI_SEND</font>(particles, NELEM, particletype, i, tag, &amp;
                    MPI_COMM_WORLD, ierr)
      end do
   endif

   <font color="#AAAAAA">! all tasks receive particletype data</font>
   source = 0
   call <font color="#DF4442">MPI_RECV</font>(p, NELEM, particletype, source, tag, &amp;
                 MPI_COMM_WORLD, stat, ierr)

   print *, 'rank= ',rank,' p(3)= ',p(3)

   <font color="#AAAAAA">! free datatype when done using it</font>
   call <font color="#DF4442">MPI_TYPE_FREE</font>(particletype, ierr)
   call <font color="#DF4442">MPI_FINALIZE</font>(ierr)
   end

</pre></td>
</tr></tbody></table>
</td></tr></tbody></table>  <!---outer table--->

<br><br>

Sample program output:
</p><pre>rank= 0   3.00 -3.00 3.00 0.25 3 1
rank= 2   3.00 -3.00 3.00 0.25 3 1
rank= 1   3.00 -3.00 3.00 0.25 3 1
rank= 3   3.00 -3.00 3.00 0.25 3 1
</pre>
</ul>

<!--------------------------------------------------------------------------->

<a name="Group_Management_Routines"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Group and Communicator Management Routines</span></td>
</tr></tbody></table>
<p><br>

<img src="./Message Passing Interface (MPI)_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Groups vs. Communicators:</span>
</p><ul>
<p>
</p><li>A group is an ordered set of processes. Each process in a group is 
    associated with a unique integer rank. Rank values start at zero and go 
    to N-1, where N is the number of processes in the group. 
    In MPI, a group is represented within system memory as an object.
    It is accessible to the programmer only by a "handle". A group is always 
    associated with a communicator object. 
<p>
</p></li><li>A communicator encompasses a group of processes that may communicate with
    each other.  All MPI messages must specify a communicator.  In the
    simplest sense, the communicator is an extra "tag" that must be included
    with MPI calls.
    Like groups, communicators are represented within system memory as 
    objects and are accessible to the programmer only by "handles". 
    For example, the handle for the communicator that comprises all tasks
    is MPI_COMM_WORLD.
<p>
</p></li><li>From the programmer's perspective, a group and a communicator are one.
    The group routines are primarily used to specify which processes should
    be used to construct a communicator.  
</li></ul>
<p>

<img src="./Message Passing Interface (MPI)_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Primary Purposes of Group and Communicator Objects:</span>
</p><ol>
<p>
</p><li>Allow you to organize tasks, based upon function, into task groups.
<p>
</p></li><li>Enable Collective Communications operations across a subset of 
    related tasks. 
<p>
</p></li><li>Provide basis for implementing user defined virtual topologies
<p>
</p></li><li>Provide for safe communications
</li></ol>
<p>

<img src="./Message Passing Interface (MPI)_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Programming Considerations and Restrictions:</span>
</p><ul>
<p>
</p><li>Groups/communicators are dynamic - they can be created and destroyed
    during program execution.
<p>
</p></li><li>Processes may be in more than one group/communicator.  They will have
    a unique rank within each group/communicator.
<p>
</p></li><li>MPI provides over 40 routines related to groups, communicators, 
    and virtual topologies.  
<p>
</p></li><li>Typical usage:
    <ol>
    <li>Extract handle of global group from MPI_COMM_WORLD using MPI_Comm_group
    </li><li>Form new group as a subset of global group using MPI_Group_incl
    </li><li>Create new communicator for new group using MPI_Comm_create
    </li><li>Determine new rank in new communicator using MPI_Comm_rank
    </li><li>Conduct communications using any MPI message passing routine
    </li><li>When finished, free up new communicator and group (optional) using 
        MPI_Comm_free and MPI_Group_free
    </li></ol>
<p>
<img src="./Message Passing Interface (MPI)_files/comm_group600pix.gif" width="511" height="600" border="0">
</p></li></ul>

<p></p><hr><p>

</p><h2>Group and Communicator Management Routines</h2>

<ul>
<p>
Create two different process groups for separate collective communications
exchange.  Requires creating new communicators also.
</p><p>
<table border="1" cellpadding="15" cellspacing="0" width="90%"><tbody><tr><td> <!---outer table--->
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tbody><tr>
<td width="30" bgcolor="FOF5FE" colspan="2" align="center"><img src="./Message Passing Interface (MPI)_files/page01.gif"></td>
<td bgcolor="FOF5FE"><b><br>&nbsp;&nbsp;&nbsp;&nbsp;
C Language - Group and Communicator Example
</b><p></p></td>

</tr><tr valign="top"><td colspan="3"><p></p></td>

</tr><tr valign="top">
<td width="30"><pre><font color="#AAAAAA"> 1<br> 2<br> 3<br> 4<br> 5<br> 6<br> 7<br> 8<br> 9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43</font></pre></td>

<td width="1" bgcolor="#7099cc"></td>

<td><pre>   #include <font color="DF4442">"mpi.h"</font>
   #include &lt;stdio.h&gt;
   #define NPROCS 8

   main(int argc, char *argv[])  {
   int        rank, new_rank, sendbuf, recvbuf, numtasks,
              ranks1[4]={0,1,2,3}, ranks2[4]={4,5,6,7};
   <font color="DF4442">MPI_Group  orig_group, new_group</font>;   <font color="#AAAAAA">// required variables</font>
   <font color="DF4442">MPI_Comm   new_comm</font>;   <font color="#AAAAAA">// required variable</font>

   <font color="DF4442">MPI_Init</font>(&amp;argc,&amp;argv);
   <font color="DF4442">MPI_Comm_rank</font>(MPI_COMM_WORLD, &amp;rank);
   <font color="DF4442">MPI_Comm_size</font>(MPI_COMM_WORLD, &amp;numtasks);

   if (numtasks != NPROCS) {
     printf("Must specify MP_PROCS= %d. Terminating.\n",NPROCS);
     <font color="DF4442">MPI_Finalize</font>();
     exit(0);
     }

   sendbuf = rank;

   <font color="#AAAAAA">// extract the original group handle</font>
   <font color="DF4442">MPI_Comm_group</font>(MPI_COMM_WORLD, &amp;orig_group);

   <font color="#AAAAAA">//  divide tasks into two distinct groups based upon rank</font>
   if (rank &lt; NPROCS/2) {
     <font color="DF4442">MPI_Group_incl</font>(orig_group, NPROCS/2, ranks1, &amp;new_group);
     }
   else {
     <font color="DF4442">MPI_Group_incl</font>(orig_group, NPROCS/2, ranks2, &amp;new_group);
     }

   <font color="#AAAAAA">// create new new communicator and then perform collective communications</font>
   <font color="DF4442">MPI_Comm_create</font>(MPI_COMM_WORLD, new_group, &amp;new_comm);
   <font color="DF4442">MPI_Allreduce</font>(&amp;sendbuf, &amp;recvbuf, 1, MPI_INT, MPI_SUM, new_comm);

   <font color="#AAAAAA">// get rank in new group</font>
   <font color="DF4442">MPI_Group_rank</font> (new_group, &amp;new_rank);
   printf("rank= %d newrank= %d recvbuf= %d\n",rank,new_rank,recvbuf);

   <font color="DF4442">MPI_Finalize</font>();
   }

</pre></td>
</tr></tbody></table>
</td></tr></tbody></table>  <!---outer table--->

<br><br><br>

<table border="1" cellpadding="15" cellspacing="0" width="90%"><tbody><tr><td> <!---outer table--->
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tbody><tr>
<td width="30" bgcolor="FOF5FE" colspan="2" align="center"><img src="./Message Passing Interface (MPI)_files/page01.gif"></td>
<td bgcolor="FOF5FE"><b><br>&nbsp;&nbsp;&nbsp;&nbsp;
Fortran - Group and Communicator Example
</b><p></p></td>

</tr><tr valign="top"><td colspan="3"><p></p></td>

</tr><tr valign="top">
<td width="30"><pre><font color="#AAAAAA"> 1<br> 2<br> 3<br> 4<br> 5<br> 6<br> 7<br> 8<br> 9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42</font></pre></td>

<td width="1" bgcolor="#7099cc"></td>

<td><pre>   program group
   include <font color="DF4442">'mpif.h'</font>

   integer NPROCS
   parameter(NPROCS=8)
   integer rank, new_rank, sendbuf, recvbuf, numtasks
   integer ranks1(4), ranks2(4), ierr
   integer <font color="DF4442">orig_group, new_group, new_comm</font>   <font color="#AAAAAA">! required variables</font>
   data ranks1 /0, 1, 2, 3/, ranks2 /4, 5, 6, 7/

   call <font color="DF4442">MPI_INIT</font>(ierr)
   call <font color="DF4442">MPI_COMM_RANK</font>(MPI_COMM_WORLD, rank, ierr)
   call <font color="DF4442">MPI_COMM_SIZE</font>(MPI_COMM_WORLD, numtasks, ierr)

   if (numtasks .ne. NPROCS) then
     print *, 'Must specify NPROCS= ',NPROCS,' Terminating.'
     call <font color="DF4442">MPI_FINALIZE</font>(ierr)
     stop
   endif

   sendbuf = rank

   <font color="#AAAAAA">! extract the original group handle</font>
   call <font color="DF4442">MPI_COMM_GROUP</font>(MPI_COMM_WORLD, orig_group, ierr)

   <font color="#AAAAAA">! divide tasks into two distinct groups based upon rank</font>
   if (rank .lt. NPROCS/2) then
      call <font color="DF4442">MPI_GROUP_INCL</font>(orig_group, NPROCS/2, ranks1, new_group, ierr)
   else 
      call <font color="DF4442">MPI_GROUP_INCL</font>(orig_group, NPROCS/2, ranks2, new_group, ierr)
   endif

   <font color="#AAAAAA">! create new new communicator and then perform collective communications</font>
   call <font color="DF4442">MPI_COMM_CREATE</font>(MPI_COMM_WORLD, new_group, new_comm, ierr)
   call <font color="DF4442">MPI_ALLREDUCE</font>(sendbuf, recvbuf, 1, MPI_INTEGER, MPI_SUM, new_comm, ierr)

   <font color="#AAAAAA">! get rank in new group</font>
   call <font color="DF4442">MPI_GROUP_RANK</font>(new_group, new_rank, ierr)
   print *, 'rank= ',rank,' newrank= ',new_rank,' recvbuf= ', recvbuf

   call <font color="DF4442">MPI_FINALIZE</font>(ierr)
   end

</pre></td>
</tr></tbody></table>
</td></tr></tbody></table>  <!---outer table--->

<br><br>

Sample program output:
</p><pre>rank= 7 newrank= 3 recvbuf= 22
rank= 0 newrank= 0 recvbuf= 6
rank= 1 newrank= 1 recvbuf= 6
rank= 2 newrank= 2 recvbuf= 6
rank= 6 newrank= 2 recvbuf= 22
rank= 3 newrank= 3 recvbuf= 6
rank= 4 newrank= 0 recvbuf= 22
rank= 5 newrank= 1 recvbuf= 22
</pre>
</ul>

<!--------------------------------------------------------------------------->

<a name="Virtual_Topologies"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Virtual Topologies</span></td>
</tr></tbody></table>
<p><br>

<img src="./Message Passing Interface (MPI)_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">What Are They?</span>
</p><ul>
<p>
</p><li>In terms of MPI, a virtual topology describes a mapping/ordering of 
    MPI processes into a geometric "shape".
<p>
</p></li><li>The two main types of topologies supported by MPI are Cartesian (grid) 
    and Graph.
<p>
</p></li><li>MPI topologies are virtual - there may be no relation between the 
    physical structure of the parallel machine and the process topology.
<p>
</p></li><li>Virtual topologies are built upon MPI communicators and groups.
<p>
</p></li><li>Must be "programmed" by the application developer.
</li></ul>

<p>
<img src="./Message Passing Interface (MPI)_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Why Use Them?</span>
</p><ul>
<p>
</p><li>Convenience
    <ul>
    <li>Virtual topologies may be useful for applications with specific 
        communication patterns - patterns that match an MPI topology structure.
    </li><li>For example, a Cartesian topology might prove convenient for an
        application that requires 4-way nearest neighbor communications
        for grid based data.
    </li></ul>
<p>
</p></li><li>Communication Efficiency 
    <ul>
    <li>Some hardware architectures may impose penalties for communications
        between successively distant "nodes".
    </li><li>A particular implementation may optimize process mapping based upon the 
        physical characteristics of a given parallel machine. 
    </li><li>The mapping of processes into an MPI virtual topology is dependent upon
        the MPI implementation, and may be totally ignored.  
    </li></ul>
</li></ul>

<p>
<img src="./Message Passing Interface (MPI)_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">Example:</span>
</p><ul>
<p>A simplified mapping of processes into a Cartesian virtual topology
    appears below:
</p><p>
<img src="./Message Passing Interface (MPI)_files/Cartesian_topology.gif" width="303" height="295" border="0">
</p></ul>


<p></p><hr><p>

</p><h2>Virtual Topology Routines</h2>

<ul>
<p>
Create a 4 x 4 Cartesian topology from 16 processors and have each process 
exchange its rank with four neighbors.
</p><p>
<table border="1" cellpadding="15" cellspacing="0" width="90%"><tbody><tr><td> <!---outer table--->
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tbody><tr>
<td width="30" bgcolor="FOF5FE" colspan="2" align="center"><img src="./Message Passing Interface (MPI)_files/page01.gif"></td>
<td bgcolor="FOF5FE"><b><br>&nbsp;&nbsp;&nbsp;&nbsp;
C Language - Cartesian Virtual Topology Example
</b><p></p></td>

</tr><tr valign="top"><td colspan="3"><p></p></td>

</tr><tr valign="top">
<td width="30"><pre><font color="#AAAAAA"> 1<br> 2<br> 3<br> 4<br> 5<br> 6<br> 7<br> 8<br> 9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54</font></pre></td>

<td width="1" bgcolor="#7099cc"></td>

<td><pre>   #include <font color="#DF4442">"mpi.h"</font>
   #include &lt;stdio.h&gt;
   #define SIZE 16
   #define UP    0
   #define DOWN  1
   #define LEFT  2
   #define RIGHT 3

   main(int argc, char *argv[])  {
   int numtasks, rank, source, dest, outbuf, i, tag=1, 
      inbuf[4]={MPI_PROC_NULL,MPI_PROC_NULL,MPI_PROC_NULL,MPI_PROC_NULL,}, 
      nbrs[4], dims[2]={4,4}, 
      periods[2]={0,0}, reorder=0, coords[2];

   <font color="#DF4442">MPI_Request reqs[8]</font>;
   <font color="#DF4442">MPI_Status stats[8]</font>;
   <font color="#DF4442">MPI_Comm cartcomm</font>;   <font color="#AAAAAA">// required variable</font>

   <font color="#DF4442">MPI_Init</font>(&amp;argc,&amp;argv);
   <font color="#DF4442">MPI_Comm_size</font>(MPI_COMM_WORLD, &amp;numtasks);

   if (numtasks == SIZE) {
      <font color="#AAAAAA">// create cartesian virtual topology, get rank, coordinates, neighbor ranks</font>
      <font color="#DF4442">MPI_Cart_create</font>(MPI_COMM_WORLD, 2, dims, periods, reorder, &amp;cartcomm);
      <font color="#DF4442">MPI_Comm_rank</font>(cartcomm, &amp;rank);
      <font color="#DF4442">MPI_Cart_coords</font>(cartcomm, rank, 2, coords);
      <font color="#DF4442">MPI_Cart_shift</font>(cartcomm, 0, 1, &amp;nbrs[UP], &amp;nbrs[DOWN]);
      <font color="#DF4442">MPI_Cart_shift</font>(cartcomm, 1, 1, &amp;nbrs[LEFT], &amp;nbrs[RIGHT]);

      printf("rank= %d coords= %d %d  neighbors(u,d,l,r)= %d %d %d %d\n",
             rank,coords[0],coords[1],nbrs[UP],nbrs[DOWN],nbrs[LEFT],
             nbrs[RIGHT]);

      outbuf = rank;

      <font color="#AAAAAA">// exchange data (rank) with 4 neighbors</font>
      for (i=0; i&lt;4; i++) {
         dest = nbrs[i];
         source = nbrs[i];
         <font color="#DF4442">MPI_Isend</font>(&amp;outbuf, 1, MPI_INT, dest, tag, 
                   MPI_COMM_WORLD, &amp;reqs[i]);
         <font color="#DF4442">MPI_Irecv</font>(&amp;inbuf[i], 1, MPI_INT, source, tag, 
                   MPI_COMM_WORLD, &amp;reqs[i+4]);
         }

      <font color="#DF4442">MPI_Waitall</font>(8, reqs, stats);
   
      printf("rank= %d                  inbuf(u,d,l,r)= %d %d %d %d\n",
             rank,inbuf[UP],inbuf[DOWN],inbuf[LEFT],inbuf[RIGHT]);  }
   else
      printf("Must specify %d processors. Terminating.\n",SIZE);
   
   <font color="#DF4442">MPI_Finalize</font>();
   }

</pre></td>
</tr></tbody></table>
</td></tr></tbody></table>  <!---outer table--->

<br><br><br>

<table border="1" cellpadding="15" cellspacing="0" width="90%"><tbody><tr><td> <!---outer table--->
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tbody><tr>
<td width="30" bgcolor="FOF5FE" colspan="2" align="center"><img src="./Message Passing Interface (MPI)_files/page01.gif"></td>
<td bgcolor="FOF5FE"><b><br>&nbsp;&nbsp;&nbsp;&nbsp;
Fortran - Cartesian Virtual Topology Example
</b><p></p></td>

</tr><tr valign="top"><td colspan="3"><p></p></td>

</tr><tr valign="top">
<td width="30"><pre><font color="#AAAAAA"> 1<br> 2<br> 3<br> 4<br> 5<br> 6<br> 7<br> 8<br> 9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58</font></pre></td>

<td width="1" bgcolor="#7099cc"></td>

<td><pre>   program cartesian
   include <font color="#DF4442">'mpif.h'</font>

   integer SIZE, UP, DOWN, LEFT, RIGHT
   parameter(SIZE=16)
   parameter(UP=1)
   parameter(DOWN=2)
   parameter(LEFT=3)
   parameter(RIGHT=4)
   integer numtasks, rank, source, dest, outbuf, i, tag, ierr, &amp;
           inbuf(4), nbrs(4), dims(2), coords(2), periods(2), reorder
   integer <font color="#DF4442">stats(MPI_STATUS_SIZE, 8), reqs(8)</font>
   integer <font color="#DF4442">cartcomm</font>   <font color="#AAAAAA">! required variable</font>
   data inbuf /MPI_PROC_NULL,MPI_PROC_NULL,MPI_PROC_NULL,MPI_PROC_NULL/, &amp;
        dims /4,4/, tag /1/, periods /0,0/, reorder /0/ 

   call <font color="#DF4442">MPI_INIT</font>(ierr)
   call <font color="#DF4442">MPI_COMM_SIZE</font>(MPI_COMM_WORLD, numtasks, ierr)
  
   if (numtasks .eq. SIZE) then
      <font color="#AAAAAA">! create cartesian virtual topology, get rank, coordinates, neighbor ranks</font>
      call <font color="#DF4442">MPI_CART_CREATE</font>(MPI_COMM_WORLD, 2, dims, periods, reorder, &amp;
                           cartcomm, ierr)
      call <font color="#DF4442">MPI_COMM_RANK</font>(cartcomm, rank, ierr)
      call <font color="#DF4442">MPI_CART_COORDS</font>(cartcomm, rank, 2, coords, ierr)
      call <font color="#DF4442">MPI_CART_SHIFT</font>(cartcomm, 0, 1, nbrs(UP), nbrs(DOWN), ierr)
      call <font color="#DF4442">MPI_CART_SHIFT</font>(cartcomm, 1, 1, nbrs(LEFT), nbrs(RIGHT), ierr)

      write(*,20) rank,coords(1),coords(2),nbrs(UP),nbrs(DOWN), &amp;
                  nbrs(LEFT),nbrs(RIGHT)

      <font color="#AAAAAA">! exchange data (rank) with 4 neighbors</font>
      outbuf = rank
      do i=1,4
         dest = nbrs(i)
         source = nbrs(i)
         call <font color="#DF4442">MPI_ISEND</font>(outbuf, 1, MPI_INTEGER, dest, tag, &amp;
                       MPI_COMM_WORLD, reqs(i), ierr)
         call <font color="#DF4442">MPI_IRECV</font>(inbuf(i), 1, MPI_INTEGER, source, tag, &amp;
                       MPI_COMM_WORLD, reqs(i+4), ierr)
      enddo

      call <font color="#DF4442">MPI_WAITALL</font>(8, reqs, stats, ierr)

      write(*,30) rank,inbuf

   else
     print *, 'Must specify',SIZE,' processors.  Terminating.' 
   endif

   call <font color="#DF4442">MPI_FINALIZE</font>(ierr)

   20 format('rank= ',I3,' coords= ',I2,I2, &amp;
             ' neighbors(u,d,l,r)= ',I3,I3,I3,I3 )
   30 format('rank= ',I3,'                 ', &amp;
             ' inbuf(u,d,l,r)= ',I3,I3,I3,I3 )

   end

</pre></td>
</tr></tbody></table>
</td></tr></tbody></table>  <!---outer table---> 

<br><br>

Sample program output: (partial)
</p><pre>rank=   0 coords=  0 0 neighbors(u,d,l,r)=  -1  4 -1  1
rank=   0                  inbuf(u,d,l,r)=  -1  4 -1  1
rank=   8 coords=  2 0 neighbors(u,d,l,r)=   4 12 -1  9
rank=   8                  inbuf(u,d,l,r)=   4 12 -1  9
rank=   1 coords=  0 1 neighbors(u,d,l,r)=  -1  5  0  2
rank=   1                  inbuf(u,d,l,r)=  -1  5  0  2
rank=  13 coords=  3 1 neighbors(u,d,l,r)=   9 -1 12 14
rank=  13                  inbuf(u,d,l,r)=   9 -1 12 14
...
...
rank=   3 coords=  0 3 neighbors(u,d,l,r)=  -1  7  2 -1
rank=   3                  inbuf(u,d,l,r)=  -1  7  2 -1
rank=  11 coords=  2 3 neighbors(u,d,l,r)=   7 15 10 -1
rank=  11                  inbuf(u,d,l,r)=   7 15 10 -1
rank=  10 coords=  2 2 neighbors(u,d,l,r)=   6 14  9 11
rank=  10                  inbuf(u,d,l,r)=   6 14  9 11
rank=   9 coords=  2 1 neighbors(u,d,l,r)=   5 13  8 10
rank=   9                  inbuf(u,d,l,r)=   5 13  8 10
</pre>
</ul>

<!--------------------------------------------------------------------------->

<a name="MPI2-3"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">A Brief Word on MPI-2 and MPI-3</span></td>
</tr></tbody></table>
<p><br>

<img src="./Message Passing Interface (MPI)_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">MPI-2:</span>
</p><ul>
<p>
</p><li>Intentionally, the MPI-1 specification did not address several
    "difficult" issues.  For reasons of expediency, these issues
    were deferred to a second specification, called MPI-2 in 1998.
<p>
</p></li><li>MPI-2 was a major revision to MPI-1 adding new functionality and
    corrections.
<p>
</p></li><li>Key areas of new functionality in MPI-2:
<ul>
<p>
</p><li><b>Dynamic Processes</b> - extensions that remove the static process model
    of MPI.  Provides routines to create new processes after job startup.
<p>
</p></li><li><b>One-Sided Communications</b> - provides routines for one directional 
    communications.  Include shared memory operations (put/get) and
    remote accumulate operations.
<p>
</p></li><li><b>Extended Collective Operations</b> - allows for the
    application of collective operations to inter-communicators
<p>
</p></li><li><b>External Interfaces</b> - defines routines that allow developers to
    layer on top of MPI, such as for debuggers and profilers.
<p>
</p></li><li><b>Additional Language Bindings</b> - describes C++ bindings and discusses
    Fortran-90 issues.
<p>
</p></li><li><b>Parallel I/O</b> - describes MPI support for parallel I/O.
</li></ul>
</li></ul>
<p>

<img src="./Message Passing Interface (MPI)_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">MPI-3:</span>
</p><ul>
<p>
</p><li>The MPI-3 standard was adopted in 2012, and contains significant extensions to 
    MPI-1 and MPI-2 functionality including: 
<ul>
<p>
</p><li><b>Nonblocking Collective Operations</b> - permits tasks in a collective to
    perform operations without blocking, possibly offering performance improvements.
<p>
</p></li><li><b>New One-sided Communication Operations</b> - to better handle different
    memory models.
<p>
</p></li><li><b>Neighborhood Collectives</b> - extends the distributed graph and Cartesian 
    process topologies with additional communication power.
<p>
</p></li><li><b>Fortran 2008 Bindings</b> - expanded from Fortran90 bindings
<p>
</p></li><li><b>MPIT Tool Interface</b> - allows the MPI implementation to expose certain 
   internal variables, counters, and other states to the user (most likely 
   performance tools).
<p>
</p></li><li><b>Matched Probe</b> - fixes an old bug in MPI-2 where one could not probe for
    messages in a multi-threaded environment.
</li></ul>
</li></ul>
<p>

<img src="./Message Passing Interface (MPI)_files/arrowBullet.gif" align="top" hspace="3">
<span class="heading3">More Information on MPI-2 and MPI-3:</span>
</p><ul>
<li>MPI Standard documents:
    <a href="http://www.mpi-forum.org/docs/" target="_blank">
    http://www.mpi-forum.org/docs/</a>
</li></ul>

<!--------------------------------------------------------------------------->

<a name="Exercise3"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">MPI Exercise 3</span></td>
</tr></tbody></table>
<h2>Your Choice</h2>

<dd>
<table border="1" cellpadding="5" cellspacing="0" width="90%">
<tbody><tr valign="top"><td bgcolor="#FOF5FE" height="400">
<b><u>Overview:</u>
<ul>
<li>Login to the LC workshop cluster, if you are not already logged in
</li><li>Following the Exercise 3 instructions will take you through all sorts
    of MPI programs - pick any/all that are of interest.
</li><li>The intention is review the codes and see what's happening - not just
    compile and run.
</li><li>Several codes provide serial examples for a comparison with the parallel
    MPI versions.
</li><li>Check out the "bug" programs. 
</li></ul>
</b><p><b>
<img src="./Message Passing Interface (MPI)_files/point02.jpg" width="100" heigth="45" border="0">
<a href="https://computing.llnl.gov/tutorials/mpi/exercise.html#Exercise3" target="_blank">GO TO THE EXERCISE HERE</a>
</b>
</p></td></tr></tbody></table>
</dd>

<br><br><br><br>

<p></p><hr><p>

<font size="+1"><b>This completes the tutorial.</b></font> 
</p><p>
<table border="0" cellpadding="0" cellspacing="0">
<tbody><tr valign="top">
<td><a href="https://computing.llnl.gov/tutorials/evaluation/index.html" target="_blank">
    <img src="./Message Passing Interface (MPI)_files/evaluationForm.gif" border="0"></a> &nbsp; &nbsp; &nbsp;</td>
<td>Please complete the online evaluation form - unless you are doing the exercise,
    in which case please complete it at the end of the exercises.</td>
</tr>
</tbody></table>
</p><p>
<font size="+1"><b>Where would you like to go now?</b></font>
</p><ul>
<li><a href="https://computing.llnl.gov/tutorials/mpi/exercise.html#Exercise3" target="_blank">Exercise 3</a>
</li><li><a href="https://computing.llnl.gov/tutorials/agenda/index.html">Agenda</a>
</li><li><a href="https://computing.llnl.gov/tutorials/mpi/#top">Back to the top</a>
</li></ul>

<!--------------------------------------------------------------------------->

<a name="References"> <br><br> </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">References and More Information</span></td>
</tr></tbody></table>
<br>

<ul>
<li>Original Author: Blaise Barney; Contact: <a href="mailto:hpc-tutorials@llnl.gov">hpc-tutorials@llnl.gov</a>, Livermore
    Computing.
<p>
</p></li><li>MPI Standard documents:
<br><a href="http://www.mpi-forum.org/docs/" target="_blank">
    http://www.mpi-forum.org/docs/</a>
<p>
</p></li><li>"Using MPI", Gropp, Lusk and Skjellum. MIT Press, 1994.
<p>
</p></li><li>MPI Tutorials:
<br><a href="http://www.mcs.anl.gov/research/projects/mpi/tutorial/" target="_blank">www.mcs.anl.gov/research/projects/mpi/tutorial</a>
<p>
</p></li><li>Livermore Computing specific information:
    <ul>
    <li>Linux Clusters Overview tutorial
    <br><a href="https://computing.llnl.gov/tutorials/linux_clusters" target="_blank">computing.llnl.gov/tutorials/linux_clusters</a>
    </li></ul>
<!---------------------------------------------------------------
<P>
<LI>IBM Parallel Environment Manuals
<BR><A HREF=http://publib.boulder.ibm.com/infocenter/clresctr/vxrx/index.jsp
    TARGET=_blank>
    publib.boulder.ibm.com/infocenter/clresctr/vxrx/index.jsp</A>
<P>
<LI>IBM Compiler Documentation:
    <BR>Fortran:
    <A HREF=http://www-01.ibm.com/software/awdtools/fortran/ 
    TARGET=_blank9>www-01.ibm.com/software/awdtools/fortran/</A>
    <BR>C/C++:
    <A HREF=http://www-01.ibm.com/software/awdtools/xlcpp/ 
    TARGET=W33>www-01.ibm.com/software/awdtools/xlcpp</A>
<P>
<LI>"RS/6000 SP: Practical MPI Programming", Yukiya Aoyama and Jun Nakano,
    RS/6000 Technical Support Center, IBM Japan.  Available from IBM's
    Redbooks server at <A HREF=http://www.redbooks.ibm.com
    TARGET=_blank>http://www.redbooks.ibm.com</A>.
---------------------------------------------------------------->
<p>
</p></li><li>"A User's Guide to MPI", Peter S. Pacheco. Department of Mathematics,
    University of San Francisco.  
</li></ul>


<a name="AppendixA"> <br><br>  </a>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr><td bgcolor="#98ABCE">
<span class="heading1">Appendix A: MPI-1 Routine Index</span></td>
</tr></tbody></table>
<br>
<ul>
<li>These man pages were derived from the MVAPICH 0.9 implementation of MPI and 
    may differ from the man pages of other implementations.
</li><li>Not all MPI routines are shown
</li><li><b>*</b> = deprecated in MPI-2.0, replaced in MPI-3.0
</li><li>The complete MPI-3 standard (2012) defines over 430 routines.

<p>
<table border="1" cellspacing="0" cellpadding="3">
<tbody><tr valign="TOP">
<th colspan="4">Environment Management Routines</th>
</tr><tr valign="TOP">
  <td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Abort.txt">MPI_Abort</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Errhandler_create.txt">MPI_Errhandler_create</a>*
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Errhandler_free.txt">MPI_Errhandler_free</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Errhandler_get.txt">MPI_Errhandler_get</a>*
</td></tr><tr valign="TOP">
  <td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Errhandler_set.txt">MPI_Errhandler_set</a>*
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Error_class.txt">MPI_Error_class</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Error_string.txt">MPI_Error_string</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Finalize.txt">MPI_Finalize</a>
</td></tr><tr valign="TOP">
  <td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Get_processor_name.txt">MPI_Get_processor_name</a>
    </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Get_version.txt">MPI_Get_version</a>
</td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Init.txt">MPI_Init</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Initialized.txt">MPI_Initialized</a>
</td></tr><tr valign="TOP">
    <td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Wtick.txt">MPI_Wtick</a>
</td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Wtime.txt">MPI_Wtime</a>
  </td><td>&nbsp;
  </td><td>&nbsp;

</td></tr><tr valign="TOP">
<th colspan="4">Point-to-Point Communication Routines</th>
</tr><tr valign="TOP">
  <td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Bsend.txt">MPI_Bsend</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Bsend_init.txt">MPI_Bsend_init</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Buffer_attach.txt">MPI_Buffer_attach</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Buffer_detach.txt">MPI_Buffer_detach</a>
</td></tr><tr valign="TOP">
  <td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Cancel.txt">MPI_Cancel</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Get_count.txt">MPI_Get_count</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Get_elements.txt">MPI_Get_elements</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Ibsend.txt">MPI_Ibsend</a>
</td></tr><tr valign="TOP">
  <td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Iprobe.txt">MPI_Iprobe</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Irecv.txt">MPI_Irecv</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Irsend.txt">MPI_Irsend</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Isend.txt">MPI_Isend</a>
</td></tr><tr valign="TOP">
  <td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Issend.txt">MPI_Issend</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Probe.txt">MPI_Probe</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Recv.txt">MPI_Recv</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Recv_init.txt">MPI_Recv_init</a>
</td></tr><tr valign="TOP">
  <td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Request_free.txt">MPI_Request_free</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Rsend.txt">MPI_Rsend</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Rsend_init.txt">MPI_Rsend_init</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Send.txt">MPI_Send</a>
</td></tr><tr valign="TOP">
  <td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Send_init.txt">MPI_Send_init</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Sendrecv.txt">MPI_Sendrecv</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Sendrecv_replace.txt">MPI_Sendrecv_replace</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Ssend.txt">MPI_Ssend</a>
</td></tr><tr valign="TOP">
  <td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Ssend_init.txt">MPI_Ssend_init</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Start.txt">MPI_Start</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Startall.txt">MPI_Startall</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Test.txt">MPI_Test</a>
</td></tr><tr valign="TOP">
  <td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Test_cancelled.txt">MPI_Test_cancelled</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Testall.txt">MPI_Testall</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Testany.txt">MPI_Testany</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Testsome.txt">MPI_Testsome</a>
</td></tr><tr valign="TOP">
  <td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Wait.txt">MPI_Wait</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Waitall.txt">MPI_Waitall</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Waitany.txt">MPI_Waitany</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Waitsome.txt">MPI_Waitsome</a>

</td></tr><tr valign="TOP">
<th colspan="4">Collective Communication Routines</th>
</tr><tr valign="TOP">
  <td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Allgather.txt">MPI_Allgather</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Allgatherv.txt">MPI_Allgatherv</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Allreduce.txt">MPI_Allreduce</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Alltoall.txt">MPI_Alltoall</a>
</td></tr><tr valign="TOP">
  <td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Alltoallv.txt">MPI_Alltoallv</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Barrier.txt">MPI_Barrier</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Bcast.txt">MPI_Bcast</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Gather.txt">MPI_Gather</a>
</td></tr><tr valign="TOP">
  <td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Gatherv.txt">MPI_Gatherv</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Op_create.txt">MPI_Op_create</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Op_free.txt">MPI_Op_free</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Reduce.txt">MPI_Reduce</a>
</td></tr><tr valign="TOP">
  <td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Reduce_scatter.txt">MPI_Reduce_scatter</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Scan.txt">MPI_Scan</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Scatter.txt">MPI_Scatter</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Scatterv.txt">MPI_Scatterv</a>

</td></tr><tr valign="TOP">
<th colspan="4">Process Group Routines</th>
</tr><tr valign="TOP">
  <td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Group_compare.txt">MPI_Group_compare</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Group_difference.txt">MPI_Group_difference</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Group_excl.txt">MPI_Group_excl</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Group_free.txt">MPI_Group_free</a>
</td></tr><tr valign="TOP">
  <td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Group_incl.txt">MPI_Group_incl</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Group_intersection.txt">MPI_Group_intersection</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Group_range_excl.txt">MPI_Group_range_excl</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Group_range_incl.txt">MPI_Group_range_incl</a>
</td></tr><tr valign="TOP">
  <td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Group_rank.txt">MPI_Group_rank</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Group_size.txt">MPI_Group_size</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Group_translate_ranks.txt">MPI_Group_translate_ranks</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Group_union.txt">MPI_Group_union</a>

</td></tr><tr valign="TOP">
<th colspan="4">Communicators Routines</th>
</tr><tr valign="TOP">
  <td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Comm_compare.txt">MPI_Comm_compare</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Comm_create.txt">MPI_Comm_create</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Comm_dup.txt">MPI_Comm_dup</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Comm_free.txt">MPI_Comm_free</a>
</td></tr><tr valign="TOP">
  <td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Comm_group.txt">MPI_Comm_group</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Comm_rank.txt">MPI_Comm_rank</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Comm_remote_group.txt">MPI_Comm_remote_group</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Comm_remote_size.txt">MPI_Comm_remote_size</a>
</td></tr><tr valign="TOP">
  <td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Comm_size.txt">MPI_Comm_size</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Comm_split.txt">MPI_Comm_split</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Comm_test_inter.txt">MPI_Comm_test_inter</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Intercomm_create.txt">MPI_Intercomm_create</a>
</td></tr><tr valign="TOP">
  <td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Intercomm_merge.txt">MPI_Intercomm_merge</a>
  </td><td>&nbsp;
  </td><td>&nbsp;
  </td><td>&nbsp;

</td></tr><tr valign="TOP">
<th colspan="4">Derived Types Routines</th>
</tr><tr valign="TOP">
  <td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Type_commit.txt">MPI_Type_commit</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Type_contiguous.txt">MPI_Type_contiguous</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Type_extent.txt">MPI_Type_extent</a>*
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Type_free.txt">MPI_Type_free</a>
</td></tr><tr valign="TOP">
  <td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Type_hindexed.txt">MPI_Type_hindexed</a>*
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Type_hvector.txt">MPI_Type_hvector</a>*
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Type_indexed.txt">MPI_Type_indexed</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Type_lb.txt">MPI_Type_lb</a>
</td></tr><tr valign="TOP">
  <td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Type_size.txt">MPI_Type_size</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Type_struct.txt">MPI_Type_struct</a>*
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Type_ub.txt">MPI_Type_ub</a>*
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Type_vector.txt">MPI_Type_vector</a>

</td></tr><tr valign="TOP">
<th colspan="4">Virtual Topology Routines</th>
</tr><tr valign="TOP">
  <td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Cart_coords.txt">MPI_Cart_coords</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Cart_create.txt">MPI_Cart_create</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Cart_get.txt">MPI_Cart_get</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Cart_map.txt">MPI_Cart_map</a>
</td></tr><tr valign="TOP">
  <td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Cart_rank.txt">MPI_Cart_rank</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Cart_shift.txt">MPI_Cart_shift</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Cart_sub.txt">MPI_Cart_sub</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Cartdim_get.txt">MPI_Cartdim_get</a>
</td></tr><tr valign="TOP">
  <td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Dims_create.txt">MPI_Dims_create</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Graph_create.txt">MPI_Graph_create</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Graph_get.txt">MPI_Graph_get</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Graph_map.txt">MPI_Graph_map</a>
</td></tr><tr valign="TOP">
  <td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Graph_neighbors.txt">MPI_Graph_neighbors</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Graph_neighbors_count.txt">MPI_Graph_neighbors_count</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Graphdims_get.txt">MPI_Graphdims_get</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Topo_test.txt">MPI_Topo_test</a>

</td></tr><tr valign="TOP">
<th colspan="4">Miscellaneous Routines</th>
</tr><tr valign="TOP">
  <td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Address.txt">MPI_Address</a>*
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Attr_delete.txt">MPI_Attr_delete</a>*
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Attr_get.txt">MPI_Attr_get</a>*
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Attr_put.txt">MPI_Attr_put</a>*
</td></tr><tr valign="TOP">
  <td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Keyval_create.txt">MPI_Keyval_create</a>*
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Keyval_free.txt">MPI_Keyval_free</a>*
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Pack.txt">MPI_Pack</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Pack_size.txt">MPI_Pack_size</a>
</td></tr><tr valign="TOP">
  <td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Pcontrol.txt">MPI_Pcontrol</a>
  </td><td><a href="https://computing.llnl.gov/tutorials/mpi/man/MPI_Unpack.txt">MPI_Unpack</a>
  </td><td>&nbsp;
  </td><td>&nbsp;
  
</td></tr></tbody></table>
</p></li></ul>

<!-------------------------------------------------------------------------->

<script language="JavaScript">PrintFooter("UCRL-MI-133316")</script><p></p><hr><span class="footer">https://computing.llnl.gov/tutorials/mpi/<br>Last Modified: 11/25/2020 04:47:23 <a href="mailto:hpc-tutorials@llnl.gov">hpc-tutorials@llnl.gov</a><br>UCRL-MI-133316<br><br>This work was performed under the auspices of the U.S. Department of Energy by Lawrence Livermore National Laboratory under Contract DE-AC52-07NA27344.</span>

<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>
<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>
<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>
<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>




</font></body></html>